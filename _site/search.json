[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Basis Deep Learning Workshop",
    "section": "",
    "text": "13 June 2024\n\n\nOur deep learning workshop includes core concepts with hands-on PyTorch experience. You’ll dive into Machine Learning (ML) and Deep Learning (DL) basics, focusing on Artificial Neural Networks (ANN). Explore astrophysics applications and participate in interactive Q&A sessions. Learn about forward and backward passes, layers, activation functions, and loss calculation. Engage in practical exercises, such as fitting a sine function in Google Colab, and discover Tensor Playground to visualize tensor operations.\n\n\n\n\n\n\n\n\n\n\nTime\nActivity\n\n\n\n\n12:30 - 13:00\nCoffee\n\n\n13:00 - 13:05\nIntroduction to the Workshop: Welcome\n\n\n13:05 - 13:50\nIntroduction to ML and DL (with ANN)\n\n\n\nML and DL applied in Astrophysics\n\n\n\nQ&A\n\n\n13:50 - 14:00\nCoffee break\n\n\n14:00 - 14:30\nIntro: Layers, Activations, Backpropagation, Loss\n\n\n14:30 - 15:20\nWhat is the neural network and how it learns\n\n\n\nDeep Learning with Pytorch and Tensor Playground\n\n\n15:20 - 15:25\nSummary: General structure of constructing a model using DL\n\n\n15:25 - 15:40\nQ&A\n\n\n15:40 - 15:55\nPlaying with Tensor Playground\n\n\n15:55 - 16:00\nQ&A\n\n\n16:00 - 16:15\nCoffee break\n\n\n16:15 - 17:30\nHands-on session"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "basic-DL",
    "section": "",
    "text": "1. Observation of Interstellar Medium (ISM) and Molecular Clouds\n\nData Reduction and Noise Filtering: ML algorithms enhance observational data quality by reducing noise and improving the signal-to-noise ratio in data from radio, infrared, and optical telescopes.\n\nReference: Samuel, H. S., Etim, E. E., Shinggu, J. P., & Bako, B. (2023). Machine learning of Rotational spectra analysis in interstellar medium. Communication in Physical Sciences, 10(1), 172-203. [3]\n\nFeature Extraction: DL models, such as convolutional neural networks (CNNs), identify and classify structures within the ISM and molecular clouds, such as filaments and cores.\n\nReference: Samuel, H. S., Etim, E. E., Shinggu, J. P., & Bako, B. (2023). Machine learning of Rotational spectra analysis in interstellar medium. Communication in Physical Sciences, 10(1), 172-203. [3]\n\nAnomaly Detection: ML techniques detect unusual features or events within the ISM, potentially indicating new astrophysical phenomena.\n\nReference: Samuel, H. S., Etim, E. E., Shinggu, J. P., & Bako, B. (2023). Machine learning of Rotational spectra analysis in interstellar medium. Communication in Physical Sciences, 10(1), 172-203. [3]\n\n\n\n\n2. High-Mass Star Formation\n\nData Analysis: ML models analyze large datasets from observatories to identify regions of high-mass star formation by detecting specific spectral lines and emission features.\n\nReference: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. Monthly Notices of the Royal Astronomical Society, 493(4), 4808-4815. [2]\n\nPattern Recognition: DL algorithms identify patterns and correlations in the spatial and temporal distribution of high-mass star-forming regions, helping to understand the underlying physical processes.\n\nReference: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. Monthly Notices of the Royal Astronomical Society, 493(4), 4808-4815. [2]\n\nPredictive Modeling: ML models predict the future evolution of star-forming regions based on current observations.\n\nReference: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. Monthly Notices of the Royal Astronomical Society, 493(4), 4808-4815. [2]\n\n\n\n\n3. Simulation of High-Mass Star Formation\n\nAccelerated Simulations: ML techniques, such as generative adversarial networks (GANs), enhance the efficiency of high-mass star formation simulations by generating high-fidelity results from lower-resolution inputs.\n\nReference: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. Astronomy & Astrophysics, 621, A51. [5]\n\nParameter Space Exploration: ML models explore the vast parameter space of star formation simulations to identify optimal conditions and initial configurations for high-mass star formation.\n\nReference: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. Astronomy & Astrophysics, 621, A51. [5]\n\n\n\n\n4. Astrochemistry\n\nChemical Abundance Mapping: ML models analyze spectral data to map the distribution of various molecules within molecular clouds and other astrophysical environments.\n\nReference: Xue, B.-X., Barbatti, M., & Dral, P. (2020). Machine learning for absorption cross sections. Journal of Physical Chemistry A, 124(35), 7199-7210. [4]\n\nReaction Network Analysis: DL helps in understanding complex chemical networks by identifying key reactions and pathways in the formation and destruction of molecules in space.\n\nReference: Villadsen, T., Ligterink, N. F. W., & Andersen, M. (2022). Predicting binding energies of astrochemically relevant molecules via machine learning. Astronomy & Astrophysics, 666, A45. [4]\n\nAnomaly Detection: ML algorithms identify unusual chemical compositions or rare molecules, providing insights into unique astrochemical processes.\n\nReference: Lee, K. L. K., Patterson, J., Burkhardt, A. M., Vankayalapati, V., & McCarthy, M. C. (2021). Machine learning of interstellar chemical inventories. Astrophysical Journal Letters, 917(1), L6. [4]\n\n\n\n\n5. Stellar Feedback\n\nSimulation and Modeling: DL accelerates simulations of stellar feedback processes, such as supernova explosions and stellar winds, by learning from high-resolution simulations and predicting outcomes for new scenarios.\n\nReference: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. Monthly Notices of the Royal Astronomical Society, 493(4), 4808-4815. [2]\n\nImpact Analysis: ML techniques assess the impact of stellar feedback on surrounding ISM and star formation by analyzing observational data and simulation outputs.\n\nReference: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. Monthly Notices of the Royal Astronomical Society, 493(4), 4808-4815. [2]\n\n\n\n\n6. Galaxy Formation\n\nMorphological Classification: CNNs classify galaxies based on their shapes and structures from large-scale survey images, aiding in the study of galaxy formation and evolution.\n\nReference: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. Astronomy & Astrophysics, 621, A51. [5]\n\nMerger Detection: ML algorithms detect galaxy mergers and interactions in observational data and simulations, which are crucial for understanding galaxy formation processes.\n\nReference: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. Astronomy & Astrophysics, 621, A51. [5]\n\nPredictive Modeling: DL models predict the future evolution of galaxies based on current observations and simulations, helping to test theories of galaxy formation.\n\nReference: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. Astronomy & Astrophysics, 621, A51. [5]"
  },
  {
    "objectID": "index.html#purpose-of-small-initial-weights",
    "href": "index.html#purpose-of-small-initial-weights",
    "title": "1. Initialize the weights and biases",
    "section": "",
    "text": "Avoid Exploding/Vanishing Activations: Large weights can cause activations to grow exponentially (exploding), while small weights can cause activations to shrink exponentially (vanishing), both hindering learning.\nBreak Symmetry: Random initialization breaks the symmetry where all neurons compute the same gradient, allowing them to learn different features.\nGradient Flow: Proper scaling helps maintain healthy gradient flow during backpropagation, preventing gradients from exploding or vanishing."
  },
  {
    "objectID": "index.html#weights-initialization",
    "href": "index.html#weights-initialization",
    "title": "1. Initialize the weights and biases",
    "section": "",
    "text": "Random Initialization: Weights are typically initialized randomly. This helps in breaking the symmetry and ensures that neurons in different layers learn different features.\n\n\n\n\nAvoid Exploding/Vanishing Activations: Large weights can cause activations to grow exponentially (exploding), while small weights can cause activations to shrink exponentially (vanishing), both hindering learning.\nBreak Symmetry: Random initialization breaks the symmetry where all neurons compute the same gradient, allowing them to learn different features.\nGradient Flow: Proper scaling helps maintain healthy gradient flow during backpropagation, preventing gradients from exploding or vanishing."
  },
  {
    "objectID": "index.html#bias-initialization",
    "href": "index.html#bias-initialization",
    "title": "1. Initialize the weights and biases",
    "section": "",
    "text": "Common Practice: Biases are often initialized to zero.\nReasoning:\n\nStarting with zero biases does not break the symmetry as weights are already randomized.\nZero initialization for biases can simplify the initial forward pass without affecting the ability of the network to learn.\n\nNon-Zero Initialization: In some specific cases, biases might be initialized to small positive values (e.g., 0.01) to ensure that all neurons in a ReLU network are initially active and contributing to the gradient."
  },
  {
    "objectID": "index.html#loss-mean-squared-error",
    "href": "index.html#loss-mean-squared-error",
    "title": "1. Initialize the weights and biases",
    "section": "loss (Mean Squared Error)",
    "text": "loss (Mean Squared Error)\n\n# 3. Compute the loss (Mean Squared Error)\ndef compute_loss(Y, A3):\n    m = Y.shape[1]\n    loss = (1/m) * np.sum((A3 - Y) ** 2)\n    return loss\n# 4. Define the backward pass\ndef backward_pass(X, Y, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3):\n    m = Y.shape[1]\n    \n    dZ3 = A3 - Y\n    dW3 = (1/m) * np.dot(dZ3, A2.T)\n    db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n    \n    dA2 = np.dot(W3.T, dZ3)\n    dZ2 = dA2 * A2 * (1 - A2)\n    dW2 = (1/m) * np.dot(dZ2, A1.T)\n    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n    \n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = dA1 * A1 * (1 - A1)\n    dW1 = (1/m) * np.dot(dZ1, X.T)\n    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n    \n    return dW1, db1, dW2, db2, dW3, db3\n# Update the parameters\ndef update_parameters(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate):\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n    W3 = W3 - learning_rate * dW3\n    b3 = b3 - learning_rate * db3\n    \n    return W1, b1, W2, b2, W3, b3\n# Example usage\ninput_size = 3\nhidden_size1 = 4\nhidden_size2 = 3\noutput_size = 1\nlearning_rate = 0.01\n\n# Initialize parameters\nW1, b1, W2, b2, W3, b3 = initialize_parameters(input_size, hidden_size1, hidden_size2, output_size)\n\n# Dummy input and output\nX = np.random.randn(input_size, 5)  # 5 examples\nY = np.random.randn(output_size, 5)\n\n# Forward pass\nZ1, A1, Z2, A2, Z3, A3 = forward_pass(X, W1, b1, W2, b2, W3, b3)\n\n# Compute loss\nloss = compute_loss(Y, A3)\nprint(f\"Loss: {loss}\")\n\n# Backward pass\ndW1, db1, dW2, db2, dW3, db3 = backward_pass(X, Y, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3)\n\n# Update parameters\nW1, b1, W2, b2, W3, b3 = update_parameters(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate)"
  },
  {
    "objectID": "index.html#compute-the-loss-mean-squared-error",
    "href": "index.html#compute-the-loss-mean-squared-error",
    "title": "1. Initialize the weights and biases",
    "section": "3. Compute the loss (Mean Squared Error)",
    "text": "3. Compute the loss (Mean Squared Error)\n\n# 3. Compute the loss (Mean Squared Error)\ndef compute_loss(Y, A3):\n    m = Y.shape[1]\n    loss = (1/m) * np.sum((A3 - Y) ** 2)\n    return loss\n# 4. Define the backward pass\ndef backward_pass(X, Y, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3):\n    m = Y.shape[1]\n    \n    dZ3 = A3 - Y\n    dW3 = (1/m) * np.dot(dZ3, A2.T)\n    db3 = (1/m) * np.sum(dZ3, axis=1, keepdims=True)\n    \n    dA2 = np.dot(W3.T, dZ3)\n    dZ2 = dA2 * A2 * (1 - A2)\n    dW2 = (1/m) * np.dot(dZ2, A1.T)\n    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n    \n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = dA1 * A1 * (1 - A1)\n    dW1 = (1/m) * np.dot(dZ1, X.T)\n    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n    \n    return dW1, db1, dW2, db2, dW3, db3\n# Update the parameters\ndef update_parameters(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate):\n    W1 = W1 - learning_rate * dW1\n    b1 = b1 - learning_rate * db1\n    W2 = W2 - learning_rate * dW2\n    b2 = b2 - learning_rate * db2\n    W3 = W3 - learning_rate * dW3\n    b3 = b3 - learning_rate * db3\n    \n    return W1, b1, W2, b2, W3, b3\n# Example usage\ninput_size = 3\nhidden_size1 = 4\nhidden_size2 = 3\noutput_size = 1\nlearning_rate = 0.01\n\n# Initialize parameters\nW1, b1, W2, b2, W3, b3 = initialize_parameters(input_size, hidden_size1, hidden_size2, output_size)\n\n# Dummy input and output\nX = np.random.randn(input_size, 5)  # 5 examples\nY = np.random.randn(output_size, 5)\n\n# Forward pass\nZ1, A1, Z2, A2, Z3, A3 = forward_pass(X, W1, b1, W2, b2, W3, b3)\n\n# Compute loss\nloss = compute_loss(Y, A3)\nprint(f\"Loss: {loss}\")\n\n# Backward pass\ndW1, db1, dW2, db2, dW3, db3 = backward_pass(X, Y, Z1, A1, Z2, A2, Z3, A3, W1, W2, W3)\n\n# Update parameters\nW1, b1, W2, b2, W3, b3 = update_parameters(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate)"
  },
  {
    "objectID": "index.html#importance-of-setting-a-random-seed",
    "href": "index.html#importance-of-setting-a-random-seed",
    "title": "1. Initialize the weights and biases",
    "section": "",
    "text": "Setting a random seed with np.random.seed(1) in NumPy is crucial for:\n\nReproducibility: Ensures the same random numbers are generated every time, enabling debugging and result verification.\nConsistency: Provides consistent initialization of neural network weights and biases, leading to consistent training outcomes.\nCollaboration: Allows others to replicate experiments and achieve the same initial conditions, facilitating collaboration and reproducibility.\n\n\n\nn is the number of the input features = 2\nk nodes = hidden_size1 = 6\np nodes = hidden_size2 = 4\nq nodes = output_size =1\n\nimport numpy as np\n#Input: [m x n]\n# m is the number of training examples\n# n is the number of features in each example = input layer \n\n# 1. Initialize the weights and biases\ndef initialize_parameters(input_size, hidden_size1, hidden_size2, output_size):\n    np.random.seed(1)\n    \n    # W1: [n x k] (n nodes = input layer), first hidden layer (k nodes = hidden_size1)\n    # b1: [k x 1] Biases for the first hidden layer, k nodes  = hidden_size1\n    W1 = np.random.randn(hidden_size1, input_size) * 0.01\n    b1 = np.zeros((hidden_size1, 1))\n    \n    \n    # W2: [k x p] (k nodes = hidden_size1), second hidden layer (p nodes = hidden_size2)\n    # b2: [p x 1] Biases for the second hidden layer, p nodes = hidden_size2\n    W2 = np.random.randn(hidden_size2, hidden_size1) * 0.01\n    b2 = np.zeros((hidden_size2, 1))\n    \n    # W3: [p x q] (p nodes = hidden_size2), the output layer (q nodes  = output_size)\n    # b3: [q x 1] Biases for the output layer, q output nodes = output_size   \n    W3 = np.random.randn(output_size, hidden_size2) * 0.01\n    b3 = np.zeros((output_size, 1))\n    \n    return W1, b1, W2, b2, W3, b3\n\n# Define the sigmoid activation function\ndef sigmoid(Z):\n    return 1 / (1 + np.exp(-Z))"
  },
  {
    "objectID": "DL-detail.html",
    "href": "DL-detail.html",
    "title": "1. Initialize the weights and biases",
    "section": "",
    "text": "In deep learning, initializing weights and biases appropriately is crucial for the performance and convergence of neural networks. Here’s a more detailed look at the initialization process for both weights and biases:\n\n\n\nRandom Initialization: Weights are typically initialized randomly. This helps in breaking the symmetry and ensures that neurons in different layers learn different features.\n\n\n\n\nAvoid Exploding/Vanishing Activations: Large weights can cause activations to grow exponentially (exploding), while small weights can cause activations to shrink exponentially (vanishing), both hindering learning.\nBreak Symmetry: Random initialization breaks the symmetry where all neurons compute the same gradient, allowing them to learn different features.\nGradient Flow: Proper scaling helps maintain healthy gradient flow during backpropagation, preventing gradients from exploding or vanishing.\n\n\n\n\n\n\nCommon Practice: Biases are often initialized to zero.\nReasoning:\n\nStarting with zero biases does not break the symmetry as weights are already randomized.\nZero initialization for biases can simplify the initial forward pass without affecting the ability of the network to learn.\n\nNon-Zero Initialization: In some specific cases, biases might be initialized to small positive values (e.g., 0.01) to ensure that all neurons in a ReLU network are initially active and contributing to the gradient.\n\n\n\n\nSetting a random seed with np.random.seed(1) in NumPy is crucial for:\n\nReproducibility: Ensures the same random numbers are generated every time, enabling debugging and result verification.\nConsistency: Provides consistent initialization of neural network weights and biases, leading to consistent training outcomes.\nCollaboration: Allows others to replicate experiments and achieve the same initial conditions, facilitating collaboration and reproducibility.\n\n\n\nn is the number of the input features = 2\nk nodes = hidden_size1 = 6\np nodes = hidden_size2 = 4\nq nodes = output_size =1\n\nimport numpy as np\n#Input: [m x n]\n# m is the number of training examples\n# n is the number of features in each example = input layer \n\n# 1. Initialize the weights and biases\ndef initialize_parameters(input_size, hidden_size1, hidden_size2, output_size):\n    np.random.seed(1)\n    \n    # W1: [n x k] (n nodes = input layer), first hidden layer (k nodes = hidden_size1)\n    # b1: [k x 1] Biases for the first hidden layer, k nodes  = hidden_size1\n    W1 = np.random.randn(hidden_size1, input_size) * 0.01\n    b1 = np.zeros((hidden_size1, 1))\n    \n    \n    # W2: [k x p] (k nodes = hidden_size1), second hidden layer (p nodes = hidden_size2)\n    # b2: [p x 1] Biases for the second hidden layer, p nodes = hidden_size2\n    W2 = np.random.randn(hidden_size2, hidden_size1) * 0.01\n    b2 = np.zeros((hidden_size2, 1))\n    \n    # W3: [p x q] (p nodes = hidden_size2), the output layer (q nodes  = output_size)\n    # b3: [q x 1] Biases for the output layer, q output nodes = output_size   \n    W3 = np.random.randn(output_size, hidden_size2) * 0.01\n    b3 = np.zeros((output_size, 1))\n    \n    return W1, b1, W2, b2, W3, b3\n\n# Define the sigmoid activation function\ndef sigmoid(Z):\n    return 1 / (1 + np.exp(-Z))"
  },
  {
    "objectID": "DL-detail.html#weights-initialization",
    "href": "DL-detail.html#weights-initialization",
    "title": "1. Initialize the weights and biases",
    "section": "",
    "text": "Random Initialization: Weights are typically initialized randomly. This helps in breaking the symmetry and ensures that neurons in different layers learn different features.\n\n\n\n\nAvoid Exploding/Vanishing Activations: Large weights can cause activations to grow exponentially (exploding), while small weights can cause activations to shrink exponentially (vanishing), both hindering learning.\nBreak Symmetry: Random initialization breaks the symmetry where all neurons compute the same gradient, allowing them to learn different features.\nGradient Flow: Proper scaling helps maintain healthy gradient flow during backpropagation, preventing gradients from exploding or vanishing."
  },
  {
    "objectID": "DL-detail.html#bias-initialization",
    "href": "DL-detail.html#bias-initialization",
    "title": "1. Initialize the weights and biases",
    "section": "",
    "text": "Common Practice: Biases are often initialized to zero.\nReasoning:\n\nStarting with zero biases does not break the symmetry as weights are already randomized.\nZero initialization for biases can simplify the initial forward pass without affecting the ability of the network to learn.\n\nNon-Zero Initialization: In some specific cases, biases might be initialized to small positive values (e.g., 0.01) to ensure that all neurons in a ReLU network are initially active and contributing to the gradient."
  },
  {
    "objectID": "DL-detail.html#importance-of-setting-a-random-seed",
    "href": "DL-detail.html#importance-of-setting-a-random-seed",
    "title": "1. Initialize the weights and biases",
    "section": "",
    "text": "Setting a random seed with np.random.seed(1) in NumPy is crucial for:\n\nReproducibility: Ensures the same random numbers are generated every time, enabling debugging and result verification.\nConsistency: Provides consistent initialization of neural network weights and biases, leading to consistent training outcomes.\nCollaboration: Allows others to replicate experiments and achieve the same initial conditions, facilitating collaboration and reproducibility.\n\n\n\nn is the number of the input features = 2\nk nodes = hidden_size1 = 6\np nodes = hidden_size2 = 4\nq nodes = output_size =1\n\nimport numpy as np\n#Input: [m x n]\n# m is the number of training examples\n# n is the number of features in each example = input layer \n\n# 1. Initialize the weights and biases\ndef initialize_parameters(input_size, hidden_size1, hidden_size2, output_size):\n    np.random.seed(1)\n    \n    # W1: [n x k] (n nodes = input layer), first hidden layer (k nodes = hidden_size1)\n    # b1: [k x 1] Biases for the first hidden layer, k nodes  = hidden_size1\n    W1 = np.random.randn(hidden_size1, input_size) * 0.01\n    b1 = np.zeros((hidden_size1, 1))\n    \n    \n    # W2: [k x p] (k nodes = hidden_size1), second hidden layer (p nodes = hidden_size2)\n    # b2: [p x 1] Biases for the second hidden layer, p nodes = hidden_size2\n    W2 = np.random.randn(hidden_size2, hidden_size1) * 0.01\n    b2 = np.zeros((hidden_size2, 1))\n    \n    # W3: [p x q] (p nodes = hidden_size2), the output layer (q nodes  = output_size)\n    # b3: [q x 1] Biases for the output layer, q output nodes = output_size   \n    W3 = np.random.randn(output_size, hidden_size2) * 0.01\n    b3 = np.zeros((output_size, 1))\n    \n    return W1, b1, W2, b2, W3, b3\n\n# Define the sigmoid activation function\ndef sigmoid(Z):\n    return 1 / (1 + np.exp(-Z))"
  },
  {
    "objectID": "index.html#machine-learning-ml",
    "href": "index.html#machine-learning-ml",
    "title": "basic-DL",
    "section": "",
    "text": "Machine learning (ML) is a subset of artificial intelligence (AI) that enables machines to learn from experience and act automatically. ML algorithms can be broadly classified into three categories: supervised learning, unsupervised learning, and reinforcement learning.\n\n\nSupervised learning involves training ML algorithms on labeled datasets where both input and output parameters are known. The goal is to develop a regression or classification model that can be used on new datasets to generate predictions or draw conclusions. Supervised learning algorithms learn to map input features to output labels, allowing them to make predictions or classifications on new, unseen data.\nExamples of supervised learning algorithms include:\n\nLinear Regression: A simple algorithm used to map the linear relationship between input features and a continuous target variable.\nLogistic Regression: An extension of linear regression used for classification tasks to estimate the likelihood that an instance belongs to a specific class.\nSupport Vector Machines (SVMs): Supervised learning algorithms that can perform classification and regression tasks by finding a hyperplane that best separates classes in feature space.\nK-Nearest Neighbors (KNN): A non-parametric technique used for classification as well as regression by identifying the k most similar data points to a new data point and predicting the label of the new data point using the labels of those data points.\nDecision Trees: A type of supervised learning technique used for classification as well as regression by segmenting the data into smaller and smaller groups until each group can be classified or predicted with high degree of accuracy.\nRandom Forest: An ensemble learning method that employs a set of decision trees to make predictions by aggregating predictions from individual trees.\nNaive Bayes: A probabilistic classifier based on Bayes’ theorem used for classification tasks by assuming that the features of a data point are independent of each other.\n\n\n\n\nUnsupervised learning involves training ML algorithms on unlabeled datasets where the data has yet to be categorized or labeled. The ML system itself learns to classify and process unlabeled data to learn from its inherent structure.\nExamples of unsupervised learning algorithms include:\n\nClustering Algorithms: Techniques used to group similar data points into clusters based on their features.\nAssociation Rule Learning Algorithms: Methods used to discover patterns and relationships between variables in the data.\nPrincipal Component Analysis (PCA): A dimensionality reduction technique used to transform data into a lower-dimensional space while retaining as much variance as possible.\n\n\n\n\nReinforcement learning involves training ML algorithms through ongoing interactions between an AI system and its environment. Algorithms receive numerical scores as rewards for generating decisions and outcomes, reinforcing positive interactions and behaviors over time.\nExamples of reinforcement learning algorithms include:\n\nMarkov Decision Processes (MDPs): Model-based methods used to solve sequential decision-making problems.\nQ-Learning: A model-free method used to learn the value function of an action in a given state.\nSARSA: Another model-free method used to learn the value function of an action in a given state.\nREINFORCE Algorithm: A policy-based method used to learn the policy of an agent in a given environment.\nActor-Critic Algorithm: A hybrid method used to learn both the policy and the value function of an agent in a given environment."
  },
  {
    "objectID": "index.html#introduction-deep-learning-dl",
    "href": "index.html#introduction-deep-learning-dl",
    "title": "basic-DL",
    "section": "Introduction Deep learning (DL)",
    "text": "Introduction Deep learning (DL)\nDeep learning (DL) is a powerful subset of machine learning techniques that are inspired by the structure and function of the brain’s neural networks. At the core of deep learning are artificial neural networks (ANNs), which are computational models composed of interconnected nodes called neurons. These neurons are organized into layers, allowing the network to learn hierarchical representations of data through a process called training.\nANNs are the fundamental building blocks of deep learning models. They consist of the following key components:\n\nLayers\n\nInput Layer: This layer receives the input data, such as an image or a text sequence.\nHidden Layers: These are the intermediate layers that transform the input data into increasingly abstract representations. The depth of a neural network refers to the number of hidden layers it has.\nOutput Layer: This layer produces the final output, such as a classification or a prediction.\n\n\n\nNeurons and Activation Functions\nEach layer in an ANN is composed of neurons, which are the basic computational units. Neurons receive input from the previous layer, perform a weighted sum of these inputs, and apply an activation function to produce an output that is passed to the next layer.\nCommon activation functions include:\n\nSigmoid: \\[\\sigma(x) = \\frac{1}{1 + e^{-x}}\\] [1]\nRectified Linear Unit (ReLU): \\[f(x) = \\max(0, x)\\] [1]\nHyperbolic Tangent (Tanh): \\[\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\] [1]\n\n\n\nTraining Neural Networks\nTraining an ANN involves the following key processes:\n\nForward Propagation: Input data is passed through the network, and the output is computed.\nLoss Function: A loss function measures the difference between the predicted output and the true target value. Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.\nBackpropagation: The gradients of the loss function with respect to the weights are computed using the chain rule of calculus. This process is called backpropagation.\nOptimization: The weights of the network are updated using an optimization algorithm, such as Gradient Descent, to minimize the loss function. Popular variants of Gradient Descent include Stochastic Gradient Descent (SGD), Mini-batch Gradient Descent, Adam, and RMSprop."
  },
  {
    "objectID": "index.html#applications-of-deep-learning",
    "href": "index.html#applications-of-deep-learning",
    "title": "basic-DL",
    "section": "Applications of Deep Learning",
    "text": "Applications of Deep Learning\nDeep learning has revolutionized various fields by providing state-of-the-art performance in numerous applications, including:\n\nComputer Vision: Image classification, object detection, image segmentation (e.g., using Convolutional Neural Networks (CNNs)).\nNatural Language Processing (NLP): Language translation, sentiment analysis, text generation (e.g., using Recurrent Neural Networks (RNNs) and Transformers).\nSpeech Recognition: Voice assistants, automated transcription.\nHealthcare: Disease diagnosis from medical images, personalized treatment recommendations.\nAutonomous Vehicles: Perception and decision-making systems.\nAstrophysics: Classifying celestial objects, detecting exoplanets, analyzing the interstellar medium.\n\n\nReferences\n\nSpringer, “Deep Learning: A Comprehensive Overview on Techniques and Applications” (2021)\nBiostrand AI, “AI, ML, DL, and NLP: An Overview” (2022)\nNCBI, “Introduction to Machine Learning, Neural Networks, and Deep Learning” (2020)\nClive Maxfield, “What the FAQ are AI, ANNs, ML, DL, and DNNs?” (n.d.)\nDataCamp, “Introduction to Deep Neural Networks” (n.d.)"
  },
  {
    "objectID": "intro_pytorch.html",
    "href": "intro_pytorch.html",
    "title": "basic-DL",
    "section": "",
    "text": "Neural networks are computational models inspired by the human brain. They consist of layers of interconnected nodes (neurons) where each connection has an associated weight. The basic flow in a neural network involves:\n\nForward Propagation: Input data passes through the network layers to produce an output.\nLoss Calculation: The output is compared with the actual target to compute the loss.\nBackpropagation: The network adjusts the weights to minimize the loss.\n\n\n\n\nA neural network typically has:\n\nInput Layer: Receives the input data.\nHidden Layers: Intermediate layers that process the inputs.\nOutput Layer: Produces the final output.\n\n\n\n\nActivation functions introduce non-linearity into the network, allowing it to learn complex patterns. Common activation functions include:\n\nReLU (Rectified Linear Unit): \\[f(x) = \\max(0, x) \\]\nSigmoid: \\[\\displaystyle f(x) = \\frac{1}{1 + e^{-x}}\\]\nTanh: \\[ f(x) = \\tanh(x) \\]\n\n\n\n\nDuring forward propagation, inputs pass through each layer and activation function to produce the final output. Example of Using the Sigmoid Activation Function: \\[\\displaystyle f(x) = \\frac{1}{1 + e^{-(w*x+b)}}\\]\nwhere: \\(w\\) is the weight and \\(b\\) is the bias.\n\n\n\nThe loss function measures the difference between the predicted output and the actual target. Common loss functions include:\n\nMean Squared Error (MSE) for regression tasks.\nCross-Entropy Loss for classification tasks.\n\n\n\n\nBackpropagation updates the weights using the gradients of the loss function with respect to the weights."
  },
  {
    "objectID": "intro_pytorch.html#introduction-to-neural-networks-with-pytorch",
    "href": "intro_pytorch.html#introduction-to-neural-networks-with-pytorch",
    "title": "basic-DL",
    "section": "Introduction to Neural Networks with PyTorch",
    "text": "Introduction to Neural Networks with PyTorch\n\nPyTorch is a popular open-source deep learning framework that offers a flexible and efficient platform for building and training neural networks. This guide introduces the basic concepts of neural networks, including forward propagation, different layers, activation functions, backpropagation, and loss functions, and demonstrates how to implement these concepts using PyTorch.\n### Define the Neural Network\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.layer1 = nn.Linear(2, 64)\n        self.layer2 = nn.Linear(64, 128)\n        self.layer3 = nn.Linear(128, 64)\n        self.layer4 = nn.Linear(64, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n      x = self.relu(self.layer1(x))\n      x = self.relu(self.layer2(x))\n      x = self.relu(self.layer3(x))\n      x = self.layer4(x)\n      return x\n    \n# Instantiate the model\nmodel = NeuralNetwork()\n\n### Define Loss and Optimizer\n\ncriterion = nn.MSELoss() #Mean Square Error(MSE) or L2 loss\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n#Adam (Adaptive Moment Estimation) optimizer computes adaptive learning rates for each parameter.\n#lr=0.001: This sets the learning rate for the optimizer\n\n### Train the Model\n\nnum_epochs = 1000\n\nfor epoch in range(num_epochs):\n    model.train() #Sets the model to training mode\n    optimizer.zero_grad() #Clears old gradients to prevent accumulation from previous iterations.\n    outputs = model(x_tensor) #Performs a forward pass, generating predictions from the input data.\n    loss = criterion(outputs, y_tensor) # Calculates the loss (error) between the predictions and true targets.\n    loss.backward()  #Computes gradients of the loss with respect to the model parameters (backpropagation).\n    optimizer.step() #Updates the model parameters using the computed gradients to minimize the loss.\n\n### Evaluate the Model\n\nmodel.eval()\nwith torch.no_grad():\n    predicted = model(x_tensor).numpy()"
  },
  {
    "objectID": "tensor_playground.html",
    "href": "tensor_playground.html",
    "title": "basic-DL",
    "section": "",
    "text": "Introduction to TensorFlow Playground\nTensorFlow Playground is an interactive web-based tool designed to help users understand the inner workings of neural networks. It provides a visual and intuitive interface for experimenting with various neural network architectures and parameters without needing to write any code. This educational tool allows users to see how changes to hyperparameters and network structure affect the model’s performance on simple datasets.\n\n\nKey Features\n\n\nInteractive Visualization: Users can visualize the data, the decision boundaries formed by the neural network, and how these boundaries evolve as the network trains.\nReal-time Feedback: Observe how changing hyperparameters and network structure in real-time impacts the training process and model performance.\nEducational Tool: Ideal for students, educators, and anyone new to machine learning who wants to gain a deeper understanding of neural networks.\n\n\n\nList of Hyperparameters and Settings in TensorFlow Playground\n\nData Settings:\n\nDataset Type: Choose from a variety of pre-set datasets like spirals, circles, Gaussian, and more.\nNoise Level: Adjust the amount of noise in the dataset to see how it affects model performance.\n\nNetwork Architecture:\n\nNumber of Hidden Layers: Adjust the number of hidden layers in the neural network.\nNumber of Neurons per Layer: Control the number of neurons in each hidden layer.\n\nActivation Functions:\n\nReLU (Rectified Linear Unit)\nTanh (Hyperbolic Tangent)\nSigmoid\nLinear\n\nRegularization:\n\nRegularization Type: Choose between L1 and L2 regularization.\nRegularization Rate: Set the strength of the regularization to prevent overfitting.\n\nLearning Rate:\n\nAdjust the learning rate: Control the step size at each iteration while moving toward a minimum of the loss function.\n\nBatch Size:\n\nAdjust the batch size: Determine the number of training examples used in one iteration.\n\nProblem Type:\n\nClassification: For tasks where the output is a category.\nRegression: For tasks where the output is a continuous value.\n\nVisualization Settings:\n\nShow Training Data: Toggle the visibility of the training data points.\nShow Test Data: Toggle the visibility of the test data points.\nHeatmap: Visualize the decision boundary of the neural network.\nTraining Iterations: Set the number of iterations to train the model.\n\n\n\n\nRegularization in TensorFlow Playground\nRegularization is a technique used in machine learning to prevent overfitting by adding a penalty to the loss function. In TensorFlow Playground, two types of regularization are implemented: L1 and L2 regularization.\nL1 Regularization\nL1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients. This can be mathematically represented as: \\[ \\text{loss} = \\text{original loss} + \\lambda \\sum |w| \\] where $ $ is the regularization rate and $ w $ represents the weights. L1 regularization encourages sparsity in the model, pushing weights towards exactly zero, which can lead to simpler models with fewer active features.\nL2 Regularization\nL2 regularization adds a penalty equal to the square of the magnitude of coefficients. This can be mathematically represented as: \\[ \\text{loss} = \\text{original loss} + \\lambda \\sum w^2 \\] L2 regularization penalizes large weights more heavily but does not push them to zero, which helps in reducing the model complexity without making it sparse.\nRegularization Rate\nThe regularization rate, denoted as $ $, controls the strength of the penalty applied to the loss function. In TensorFlow Playground, the regularization rate can be adjusted to see its effect on the model’s performance. A higher regularization rate increases the penalty, which can prevent the model from overfitting but may also lead to underfitting if set too high."
  },
  {
    "objectID": "index1.html",
    "href": "index1.html",
    "title": "Basis Deep Learning Workshop",
    "section": "",
    "text": "13 June 2024\n\n\nOur deep learning workshop includes core concepts with hands-on PyTorch experience. You’ll dive into Machine Learning (ML) and Deep Learning (DL) basics, focusing on Artificial Neural Networks (ANN). Explore astrophysics applications and participate in interactive Q&A sessions. Learn about forward and backward passes, layers, activation functions, and loss calculation. Engage in practical exercises, such as fitting a sine function in Google Colab, and discover Tensor Playground to visualize tensor operations.\n\n\n\n\n\n\n\n\n\n\nTime\nActivity\n\n\n\n\n12:30 - 13:00\nCoffee\n\n\n13:00 - 13:05\nIntroduction to the Workshop: Welcome\n\n\n13:05 - 13:50\nIntroduction to ML and DL (with ANN)\n\n\n\nML and DL applied in Astrophysics\n\n\n\nQ&A\n\n\n13:50 - 14:00\nCoffee break\n\n\n14:00 - 14:30\nIntro: Layers, Activations, Backpropagation, Loss & PyTorch\n\n\n14:30 - 15:20\nWhat is the neural network and how it learns\n\n\n15:20 - 15:25\nSummary: General structure of constructing a model using DL\n\n\n15:25 - 11:40\nQ&A\n\n\n15:40 - 15:55\nPlaying with a simple exercise\n\n\n15:55 - 16:00\nQ&A\n\n\n16:00 - 16:15\nPlaying with Tensor Playground\n\n\n16:15 - 16:30\nCoffee break\n\n\n16:30 - 17:30\nHands-on session"
  },
  {
    "objectID": "index1.html#introduction",
    "href": "index1.html#introduction",
    "title": "Basis Deep Learning Workshop",
    "section": "",
    "text": "Our deep learning workshop includes core concepts with hands-on PyTorch experience. You’ll dive into Machine Learning (ML) and Deep Learning (DL) basics, focusing on Artificial Neural Networks (ANN). Explore astrophysics applications and participate in interactive Q&A sessions. Learn about forward and backward passes, layers, activation functions, and loss calculation. Engage in practical exercises, such as fitting a sine function in Google Colab, and discover Tensor Playground to visualize tensor operations."
  },
  {
    "objectID": "index1.html#timetable",
    "href": "index1.html#timetable",
    "title": "Basis Deep Learning Workshop",
    "section": "",
    "text": "Time\nActivity\n\n\n\n\n12:30 - 13:00\nCoffee\n\n\n13:00 - 13:05\nIntroduction to the Workshop: Welcome\n\n\n13:05 - 13:50\nIntroduction to ML and DL (with ANN)\n\n\n\nML and DL applied in Astrophysics\n\n\n\nQ&A\n\n\n13:50 - 14:00\nCoffee break\n\n\n14:00 - 14:30\nIntro: Layers, Activations, Backpropagation, Loss & PyTorch\n\n\n14:30 - 15:20\nWhat is the neural network and how it learns\n\n\n15:20 - 15:25\nSummary: General structure of constructing a model using DL\n\n\n15:25 - 11:40\nQ&A\n\n\n15:40 - 15:55\nPlaying with a simple exercise\n\n\n15:55 - 16:00\nQ&A\n\n\n16:00 - 16:15\nPlaying with Tensor Playground\n\n\n16:15 - 16:30\nCoffee break\n\n\n16:30 - 17:30\nHands-on session"
  },
  {
    "objectID": "DL-intro.html",
    "href": "DL-intro.html",
    "title": "basic-DL",
    "section": "",
    "text": "Machine learning (ML) is a subset of artificial intelligence (AI) that enables machines to learn from experience and act automatically. ML algorithms can be broadly classified into three categories: supervised learning, unsupervised learning, and reinforcement learning.\n\n\nSupervised learning involves training ML algorithms on labeled datasets where both input and output parameters are known. The goal is to develop a regression or classification model that can be used on new datasets to generate predictions or draw conclusions. Supervised learning algorithms learn to map input features to output labels, allowing them to make predictions or classifications on new, unseen data.\nExamples of supervised learning algorithms include:\n\nLinear Regression: A simple algorithm used to map the linear relationship between input features and a continuous target variable.\nLogistic Regression: An extension of linear regression used for classification tasks to estimate the likelihood that an instance belongs to a specific class.\nSupport Vector Machines (SVMs): Supervised learning algorithms that can perform classification and regression tasks by finding a hyperplane that best separates classes in feature space.\nK-Nearest Neighbors (KNN): A non-parametric technique used for classification as well as regression by identifying the k most similar data points to a new data point and predicting the label of the new data point using the labels of those data points.\nDecision Trees: A type of supervised learning technique used for classification as well as regression by segmenting the data into smaller and smaller groups until each group can be classified or predicted with high degree of accuracy.\nRandom Forest: An ensemble learning method that employs a set of decision trees to make predictions by aggregating predictions from individual trees.\nNaive Bayes: A probabilistic classifier based on Bayes’ theorem used for classification tasks by assuming that the features of a data point are independent of each other.\n\n\n\n\nUnsupervised learning involves training ML algorithms on unlabeled datasets where the data has yet to be categorized or labeled. The ML system itself learns to classify and process unlabeled data to learn from its inherent structure.\nExamples of unsupervised learning algorithms include:\n\nClustering Algorithms: Techniques used to group similar data points into clusters based on their features.\nAssociation Rule Learning Algorithms: Methods used to discover patterns and relationships between variables in the data.\nPrincipal Component Analysis (PCA): A dimensionality reduction technique used to transform data into a lower-dimensional space while retaining as much variance as possible.\n\n\n\n\nReinforcement learning involves training ML algorithms through ongoing interactions between an AI system and its environment. Algorithms receive numerical scores as rewards for generating decisions and outcomes, reinforcing positive interactions and behaviors over time.\nExamples of reinforcement learning algorithms include:\n\nMarkov Decision Processes (MDPs): Model-based methods used to solve sequential decision-making problems.\nQ-Learning: A model-free method used to learn the value function of an action in a given state.\nSARSA: Another model-free method used to learn the value function of an action in a given state.\nREINFORCE Algorithm: A policy-based method used to learn the policy of an agent in a given environment.\nActor-Critic Algorithm: A hybrid method used to learn both the policy and the value function of an agent in a given environment."
  },
  {
    "objectID": "DL-intro.html#machine-learning-ml",
    "href": "DL-intro.html#machine-learning-ml",
    "title": "basic-DL",
    "section": "",
    "text": "Machine learning (ML) is a subset of artificial intelligence (AI) that enables machines to learn from experience and act automatically. ML algorithms can be broadly classified into three categories: supervised learning, unsupervised learning, and reinforcement learning.\n\n\nSupervised learning involves training ML algorithms on labeled datasets where both input and output parameters are known. The goal is to develop a regression or classification model that can be used on new datasets to generate predictions or draw conclusions. Supervised learning algorithms learn to map input features to output labels, allowing them to make predictions or classifications on new, unseen data.\nExamples of supervised learning algorithms include:\n\nLinear Regression: A simple algorithm used to map the linear relationship between input features and a continuous target variable.\nLogistic Regression: An extension of linear regression used for classification tasks to estimate the likelihood that an instance belongs to a specific class.\nSupport Vector Machines (SVMs): Supervised learning algorithms that can perform classification and regression tasks by finding a hyperplane that best separates classes in feature space.\nK-Nearest Neighbors (KNN): A non-parametric technique used for classification as well as regression by identifying the k most similar data points to a new data point and predicting the label of the new data point using the labels of those data points.\nDecision Trees: A type of supervised learning technique used for classification as well as regression by segmenting the data into smaller and smaller groups until each group can be classified or predicted with high degree of accuracy.\nRandom Forest: An ensemble learning method that employs a set of decision trees to make predictions by aggregating predictions from individual trees.\nNaive Bayes: A probabilistic classifier based on Bayes’ theorem used for classification tasks by assuming that the features of a data point are independent of each other.\n\n\n\n\nUnsupervised learning involves training ML algorithms on unlabeled datasets where the data has yet to be categorized or labeled. The ML system itself learns to classify and process unlabeled data to learn from its inherent structure.\nExamples of unsupervised learning algorithms include:\n\nClustering Algorithms: Techniques used to group similar data points into clusters based on their features.\nAssociation Rule Learning Algorithms: Methods used to discover patterns and relationships between variables in the data.\nPrincipal Component Analysis (PCA): A dimensionality reduction technique used to transform data into a lower-dimensional space while retaining as much variance as possible.\n\n\n\n\nReinforcement learning involves training ML algorithms through ongoing interactions between an AI system and its environment. Algorithms receive numerical scores as rewards for generating decisions and outcomes, reinforcing positive interactions and behaviors over time.\nExamples of reinforcement learning algorithms include:\n\nMarkov Decision Processes (MDPs): Model-based methods used to solve sequential decision-making problems.\nQ-Learning: A model-free method used to learn the value function of an action in a given state.\nSARSA: Another model-free method used to learn the value function of an action in a given state.\nREINFORCE Algorithm: A policy-based method used to learn the policy of an agent in a given environment.\nActor-Critic Algorithm: A hybrid method used to learn both the policy and the value function of an agent in a given environment."
  },
  {
    "objectID": "DL-intro.html#introduction-deep-learning-dl",
    "href": "DL-intro.html#introduction-deep-learning-dl",
    "title": "basic-DL",
    "section": "Introduction Deep learning (DL)",
    "text": "Introduction Deep learning (DL)\nDeep learning (DL) is a powerful subset of machine learning techniques that are inspired by the structure and function of the brain’s neural networks. At the core of deep learning are artificial neural networks (ANNs), which are computational models composed of interconnected nodes called neurons. These neurons are organized into layers, allowing the network to learn hierarchical representations of data through a process called training.\nANNs are the fundamental building blocks of deep learning models. They consist of the following key components:\n\nLayers\n\nInput Layer: This layer receives the input data, such as an image or a text sequence.\nHidden Layers: These are the intermediate layers that transform the input data into increasingly abstract representations. The depth of a neural network refers to the number of hidden layers it has.\nOutput Layer: This layer produces the final output, such as a classification or a prediction.\n\n\n\nNeurons and Activation Functions\nEach layer in an ANN is composed of neurons, which are the basic computational units. Neurons receive input from the previous layer, perform a weighted sum of these inputs, and apply an activation function to produce an output that is passed to the next layer.\nCommon activation functions include:\n\nSigmoid: \\[\\sigma(x) = \\frac{1}{1 + e^{-x}}\\]\nRectified Linear Unit (ReLU): \\[f(x) = \\max(0, x)\\]\nHyperbolic Tangent (Tanh): \\[\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\]\n\n\n\nTraining Neural Networks\nTraining an ANN involves the following key processes:\n\nForward Propagation: Input data is passed through the network, and the output is computed.\nLoss Function: A loss function measures the difference between the predicted output and the true target value. Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.\nBackpropagation: The gradients of the loss function with respect to the weights are computed using the chain rule of calculus. This process is called backpropagation.\nOptimization: The weights of the network are updated using an optimization algorithm, such as Gradient Descent, to minimize the loss function. Popular variants of Gradient Descent include Stochastic Gradient Descent (SGD), Mini-batch Gradient Descent, Adam, and RMSprop."
  },
  {
    "objectID": "DL-intro.html#applications-of-deep-learning",
    "href": "DL-intro.html#applications-of-deep-learning",
    "title": "basic-DL",
    "section": "Applications of Deep Learning",
    "text": "Applications of Deep Learning\nDeep learning has revolutionized various fields by providing state-of-the-art performance in numerous applications, including:\n\nComputer Vision: Image classification, object detection, image segmentation (e.g., using Convolutional Neural Networks (CNNs)).\nNatural Language Processing (NLP): Language translation, sentiment analysis, text generation (e.g., using Recurrent Neural Networks (RNNs) and Transformers).\nSpeech Recognition: Voice assistants, automated transcription.\nHealthcare: Disease diagnosis from medical images, personalized treatment recommendations.\nAutonomous Vehicles: Perception and decision-making systems.\nAstrophysics: Classifying celestial objects, detecting exoplanets, analyzing the interstellar medium.\n\n\nReferences\n\nSpringer, “Deep Learning: A Comprehensive Overview on Techniques and Applications” (2021)\nBiostrand AI, “AI, ML, DL, and NLP: An Overview” (2022)\nNCBI, “Introduction to Machine Learning, Neural Networks, and Deep Learning” (2020)\nClive Maxfield, “What the FAQ are AI, ANNs, ML, DL, and DNNs?” (n.d.)\nDataCamp, “Introduction to Deep Neural Networks” (n.d.)"
  },
  {
    "objectID": "Handon2.html",
    "href": "Handon2.html",
    "title": "basic-DL",
    "section": "",
    "text": "Google colab - Hand on 2"
  },
  {
    "objectID": "Handon2.html#the-architecture-of-the-neural-network",
    "href": "Handon2.html#the-architecture-of-the-neural-network",
    "title": "basic-DL",
    "section": "The architecture of the neural network",
    "text": "The architecture of the neural network\n\nInput Size = 2\nThe input size of 2 is determined by the nature of the dataset we’re working with. The make_circles function from sklearn.datasets generates a 2-dimensional dataset where each data point has two features (coordinates in a 2D space). Therefore, the input to the neural network has to accommodate these two features, which is why the first layer (nn.Linear(2, 64)) expects an input size of 2.\n\n\nOutput Size = 1\nThe output size of 1 is determined by the task we’re trying to solve, which is a binary classification problem. The make_circles dataset labels each data point as either 0 or 1. Thus, the neural network needs to output a single value for each data point that can be interpreted as the probability of the data point belonging to class 1. The final layer (nn.Linear(64, 1)) reduces the output to a single value for each input data point.\n\n\nNetwork Architecture\nHere’s a brief breakdown of the network layers:\n\nInput Layer: self.layer1 = nn.Linear(2, 64)\n\nTakes 2 input features (the 2D coordinates).\nOutputs 64 features.\nActivation function: ReLU (self.relu).\n\nFirst Hidden Layer: self.layer2 = nn.Linear(64, 128)\n\nTakes the 64 features from the previous layer.\nOutputs 128 features.\nActivation function: ReLU.\n\nSecond Hidden Layer: self.layer3 = nn.Linear(128, 64)\n\nTakes the 128 features from the previous layer.\nOutputs 64 features.\nActivation function: ReLU.\n\nOutput Layer: self.layer4 = nn.Linear(64, 1)\n\nTakes the 64 features from the previous layer.\nOutputs a single feature, representing the predicted probability of the input belonging to class 1.\n\n\n\n\nActivation Function: ReLU\nThe ReLU (Rectified Linear Unit) activation function is applied after each hidden layer to introduce non-linearity into the model, allowing it to learn more complex patterns.\n\n# Step 3: Define the Neural Network\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.layer1 = nn.Linear(2, 64)\n        self.layer2 = nn.Linear(64, 128)\n        self.layer3 = nn.Linear(128, 64)\n        self.layer4 = nn.Linear(64, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        x = self.relu(self.layer2(x))\n        x = self.relu(self.layer3(x))\n        x = self.layer4(x)\n        return x\n\nmodel = NeuralNetwork()\n\n# Step 4: Define Loss and Optimizer\n\ncriterion = nn.BCEWithLogitsLoss()\n#BCEWithLogitsLoss: Binary Cross Entropy with Logits Loss which is suitable for binary classification tasks\n    #•  Binary Cross Entropy (BCE) Loss: Measures the difference between true labels and predicted probabilities.\n    #•  With Logits: Expects raw output (logits) from the model, not probabilities. Internally applies the sigmoid function to logits before computing the loss.\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n#Adam (Adaptive Moment Estimation) optimizer computes adaptive learning rates for each parameter.\n#lr=0.001: This sets the learning rate for the optimizer\n# Step 5: Train the Model\n\nnum_epochs = 1000\nlosses = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train)\n    loss = criterion(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n\n    if (epoch+1) % 100 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n    losses.append(loss.item())\n# Plot the training loss\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.show()\nmodel.eval()\nwith torch.no_grad():\n    train_outputs = model(X_train)\n    test_outputs = model(X_test)\n    train_pred = torch.round(torch.sigmoid(train_outputs))\n    test_pred = torch.round(torch.sigmoid(test_outputs))\n\n    train_accuracy = (train_pred.eq(y_train).sum() / float(y_train.shape[0])).item()\n    test_accuracy = (test_pred.eq(y_test).sum() / float(y_test.shape[0])).item()\n\nprint(f'Train Accuracy: {train_accuracy*100:.2f}%')\nprint(f'Test Accuracy: {test_accuracy*100:.2f}%')\n\n# Plot the results\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.title('Train Data')\nplt.scatter(X_train[:, 0], X_train[:, 1], c=train_pred[:, 0], cmap='coolwarm', alpha=0.6)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.subplot(1, 2, 2)\nplt.title('Test Data')\nplt.scatter(X_test[:, 0], X_test[:, 1], c=test_pred[:, 0], cmap='coolwarm', alpha=0.6)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.show()\n# Function 1: plot decision boundary\n\ndef plot_decision_boundary(model: torch.nn.Module, X: torch.Tensor, y: torch.Tensor):\n    \"\"\"\n    Plots decision boundaries of model predictions on X in comparison to y.\n    \"\"\"\n    # Move model and data to CPU\n    model.to(\"cpu\")\n    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n\n    # Setup grid for plotting decision boundaries\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n\n    # Prepare data for prediction\n    X_to_pred_on = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()\n\n    # Make predictions\n    model.eval()\n    with torch.no_grad():\n        y_logits = model(X_to_pred_on)\n\n    # Adjust logits to prediction labels\n    if len(torch.unique(y)) &gt; 2:\n        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)  # multi-class\n    else:\n        y_pred = torch.round(torch.sigmoid(y_logits))  # binary\n\n    # Reshape predictions and plot\n    y_pred = y_pred.reshape(xx.shape).numpy()\n    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.show()\n\n# Function 2: plot predictions\ndef plot_predictions(train_data, train_labels, test_data, test_labels, predictions=None):\n    \"\"\"\n    Plots training and test data and compares predictions if provided.\n    \"\"\"\n    plt.figure(figsize=(10, 7))\n\n    # Plot training data\n    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n\n    # Plot test data\n    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n\n    if predictions is not None:\n        # Plot predictions on test data\n        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n\n    # Display legend\n    plt.legend(prop={\"size\": 14})\n    plt.show()\n# Plot decision boundaries for training and test sets\nplt.figure(figsize=(6, 6))\nplt.title(\"Train\")\nplot_decision_boundary(model, X_train, y_train)\n\nplt.figure(figsize=(6, 6))\nplt.title(\"Test\")\nplot_decision_boundary(model, X_test, y_test)"
  },
  {
    "objectID": "DL/DL/tensor_playground.html",
    "href": "DL/DL/tensor_playground.html",
    "title": "basic-DL",
    "section": "",
    "text": "Introduction to TensorFlow Playground\nTensorFlow Playground is an interactive web-based tool designed to help users understand the inner workings of neural networks. It provides a visual and intuitive interface for experimenting with various neural network architectures and parameters without needing to write any code. This educational tool allows users to see how changes to hyperparameters and network structure affect the model’s performance on simple datasets.\n\n\nKey Features\n\n\nInteractive Visualization: Users can visualize the data, the decision boundaries formed by the neural network, and how these boundaries evolve as the network trains.\nReal-time Feedback: Observe how changing hyperparameters and network structure in real-time impacts the training process and model performance.\nEducational Tool: Ideal for students, educators, and anyone new to machine learning who wants to gain a deeper understanding of neural networks.\n\n\n\nList of Hyperparameters and Settings in TensorFlow Playground\n\nData Settings:\n\nDataset Type: Choose from a variety of pre-set datasets like spirals, circles, Gaussian, and more.\nNoise Level: Adjust the amount of noise in the dataset to see how it affects model performance.\n\nNetwork Architecture:\n\nNumber of Hidden Layers: Adjust the number of hidden layers in the neural network.\nNumber of Neurons per Layer: Control the number of neurons in each hidden layer.\n\nActivation Functions:\n\nReLU (Rectified Linear Unit)\nTanh (Hyperbolic Tangent)\nSigmoid\nLinear\n\nRegularization:\n\nRegularization Type: Choose between L1 and L2 regularization.\nRegularization Rate: Set the strength of the regularization to prevent overfitting.\n\nLearning Rate:\n\nAdjust the learning rate: Control the step size at each iteration while moving toward a minimum of the loss function.\n\nBatch Size:\n\nAdjust the batch size: Determine the number of training examples used in one iteration.\n\nProblem Type:\n\nClassification: For tasks where the output is a category.\nRegression: For tasks where the output is a continuous value.\n\nVisualization Settings:\n\nShow Training Data: Toggle the visibility of the training data points.\nShow Test Data: Toggle the visibility of the test data points.\nHeatmap: Visualize the decision boundary of the neural network.\nTraining Iterations: Set the number of iterations to train the model."
  },
  {
    "objectID": "DL/DL/about.html",
    "href": "DL/DL/about.html",
    "title": "basic-DL",
    "section": "",
    "text": "1. Observation of Interstellar Medium (ISM) and Molecular Clouds\n\nData Reduction and Noise Filtering: ML algorithms enhance observational data quality by reducing noise and improving the signal-to-noise ratio in data from radio, infrared, and optical telescopes.\n\nReference: Samuel, H. S., Etim, E. E., Shinggu, J. P., & Bako, B. (2023). Machine learning of Rotational spectra analysis in interstellar medium. Communication in Physical Sciences, 10(1), 172-203. [3]\n\nFeature Extraction: DL models, such as convolutional neural networks (CNNs), identify and classify structures within the ISM and molecular clouds, such as filaments and cores.\n\nReference: Samuel, H. S., Etim, E. E., Shinggu, J. P., & Bako, B. (2023). Machine learning of Rotational spectra analysis in interstellar medium. Communication in Physical Sciences, 10(1), 172-203. [3]\n\nAnomaly Detection: ML techniques detect unusual features or events within the ISM, potentially indicating new astrophysical phenomena.\n\nReference: Samuel, H. S., Etim, E. E., Shinggu, J. P., & Bako, B. (2023). Machine learning of Rotational spectra analysis in interstellar medium. Communication in Physical Sciences, 10(1), 172-203. [3]\n\n\n\n\n2. High-Mass Star Formation\n\nData Analysis: ML models analyze large datasets from observatories to identify regions of high-mass star formation by detecting specific spectral lines and emission features.\n\nReference: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. Monthly Notices of the Royal Astronomical Society, 493(4), 4808-4815. [2]\n\nPattern Recognition: DL algorithms identify patterns and correlations in the spatial and temporal distribution of high-mass star-forming regions, helping to understand the underlying physical processes.\n\nReference: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. Monthly Notices of the Royal Astronomical Society, 493(4), 4808-4815. [2]\n\nPredictive Modeling: ML models predict the future evolution of star-forming regions based on current observations.\n\nReference: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. Monthly Notices of the Royal Astronomical Society, 493(4), 4808-4815. [2]\n\n\n\n\n3. Simulation of High-Mass Star Formation\n\nAccelerated Simulations: ML techniques, such as generative adversarial networks (GANs), enhance the efficiency of high-mass star formation simulations by generating high-fidelity results from lower-resolution inputs.\n\nReference: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. Astronomy & Astrophysics, 621, A51. [5]\n\nParameter Space Exploration: ML models explore the vast parameter space of star formation simulations to identify optimal conditions and initial configurations for high-mass star formation.\n\nReference: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. Astronomy & Astrophysics, 621, A51. [5]\n\n\n\n\n4. Astrochemistry\n\nChemical Abundance Mapping: ML models analyze spectral data to map the distribution of various molecules within molecular clouds and other astrophysical environments.\n\nReference: Xue, B.-X., Barbatti, M., & Dral, P. (2020). Machine learning for absorption cross sections. Journal of Physical Chemistry A, 124(35), 7199-7210. [4]\n\nReaction Network Analysis: DL helps in understanding complex chemical networks by identifying key reactions and pathways in the formation and destruction of molecules in space.\n\nReference: Villadsen, T., Ligterink, N. F. W., & Andersen, M. (2022). Predicting binding energies of astrochemically relevant molecules via machine learning. Astronomy & Astrophysics, 666, A45. [4]\n\nAnomaly Detection: ML algorithms identify unusual chemical compositions or rare molecules, providing insights into unique astrochemical processes.\n\nReference: Lee, K. L. K., Patterson, J., Burkhardt, A. M., Vankayalapati, V., & McCarthy, M. C. (2021). Machine learning of interstellar chemical inventories. Astrophysical Journal Letters, 917(1), L6. [4]\n\n\n\n\n5. Stellar Feedback\n\nSimulation and Modeling: DL accelerates simulations of stellar feedback processes, such as supernova explosions and stellar winds, by learning from high-resolution simulations and predicting outcomes for new scenarios.\n\nReference: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. Monthly Notices of the Royal Astronomical Society, 493(4), 4808-4815. [2]\n\nImpact Analysis: ML techniques assess the impact of stellar feedback on surrounding ISM and star formation by analyzing observational data and simulation outputs.\n\nReference: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. Monthly Notices of the Royal Astronomical Society, 493(4), 4808-4815. [2]\n\n\n\n\n6. Galaxy Formation\n\nMorphological Classification: CNNs classify galaxies based on their shapes and structures from large-scale survey images, aiding in the study of galaxy formation and evolution.\n\nReference: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. Astronomy & Astrophysics, 621, A51. [5]\n\nMerger Detection: ML algorithms detect galaxy mergers and interactions in observational data and simulations, which are crucial for understanding galaxy formation processes.\n\nReference: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. Astronomy & Astrophysics, 621, A51. [5]\n\nPredictive Modeling: DL models predict the future evolution of galaxies based on current observations and simulations, helping to test theories of galaxy formation.\n\nReference: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. Astronomy & Astrophysics, 621, A51. [5]"
  },
  {
    "objectID": "Handon1.html",
    "href": "Handon1.html",
    "title": "basic-DL",
    "section": "",
    "text": "Google colab - Hand on 1\n\nImport Libraries: First, we need to import the necessary libraries.\n\n### Step 1: Import Libraries\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nCreate Synthetic Data: Generate some synthetic data that represents a complex function. For this example, we’ll use a sine wave with added noise.\n\n### Step 2: Create Synthetic Data\n\n# Generate synthetic data\nnp.random.seed(42)\nx = np.linspace(-2 * np.pi, 2 * np.pi, 1000)\ny = np.sin(x) + 0.1 * np.random.normal(size=x.shape)\n\n# Convert to PyTorch tensors\nx_tensor = torch.tensor(x, dtype=torch.float32).view(-1, 1)\ny_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\nplt.plot(x,y)\n\nDefine the Neural Network: Create a neural network with three hidden layers.\n\n### Step 3: Define the Neural Network\nrelu=True #none-linear activation function\n#relu=False #none-linear activation function\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.layer1 = nn.Linear(1, 64)\n        self.layer2 = nn.Linear(64, 128)\n        self.layer3 = nn.Linear(128, 64)\n        self.layer4 = nn.Linear(64, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n      if relu:\n        x = self.relu(self.layer1(x))\n        x = self.relu(self.layer2(x))\n        x = self.relu(self.layer3(x))\n        x = self.layer4(x)\n        return x\n      else:\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\nmodel = NeuralNetwork()\n\nDefine Loss and Optimizer: Choose a loss function and an optimizer.\n\n### Step 4: Define Loss and Optimizer\n\ncriterion = nn.MSELoss() #Mean Square Error(MSE) or L2 loss\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n#Adam (Adaptive Moment Estimation) optimizer computes adaptive learning rates for each parameter.\n#lr=0.001: This sets the learning rate for the optimizer\n\nTrain the Model: Train the neural network on the synthetic data.\n\n### Step 5: Train the Model\n\nnum_epochs = 1000\nlosses = []\n\nfor epoch in range(num_epochs):\n    model.train() #Sets the model to training mode\n    optimizer.zero_grad() #Clears old gradients to prevent accumulation from previous iterations.\n    outputs = model(x_tensor) #Performs a forward pass, generating predictions from the input data.\n    loss = criterion(outputs, y_tensor) # Calculates the loss (error) between the predictions and true targets.\n    loss.backward()  #Computes gradients of the loss with respect to the model parameters (backpropagation).\n    optimizer.step() #Updates the model parameters using the computed gradients to minimize the loss.\n\n    if (epoch+1) % 100 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n    losses.append(loss.item())\n# Plot the training loss\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.show()\n\nEvaluate the Model: Evaluate the model’s performance.\n\n### Step 6: Evaluate the Model\n\nmodel.eval()\nwith torch.no_grad():\n    predicted = model(x_tensor).numpy()\n\n# Plot the results\nplt.plot(x, y, label='Original Data')\nplt.plot(x, predicted, label='Fitted Data')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "Basic_DL/Basic_DL/about1.html",
    "href": "Basic_DL/Basic_DL/about1.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Basic_DL/Basic_DL/Handon3.html",
    "href": "Basic_DL/Basic_DL/Handon3.html",
    "title": "basic-DL",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport warnings\n\n# Suppress all RuntimeWarnings\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\n\n# Generate the dataset\nX, y = make_moons(n_samples=1000, noise=0.03, random_state=42)\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Convert the data to PyTorch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Change to (N, 1)\ny_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)  # Change to (N, 1)\n\n\n# Define the neural network model\nclass MoonModel(nn.Module):\n    def __init__(self):\n        super(MoonModel, self).__init__()\n        self.layer_1 = nn.Linear(2, 10)\n        self.layer_2 = nn.Linear(10, 10)\n        self.layer_3 = nn.Linear(10, 10)\n        self.output = nn.Linear(10, 1)  # Single output neuron\n\n    def forward(self, x):\n        x = torch.relu(self.layer_1(x))\n        x = torch.relu(self.layer_2(x))\n        x = torch.relu(self.layer_3(x))\n        x = self.output(x)\n        return x\n\nmodel = MoonModel()\n\n\n# Define the loss function and the optimizer\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n\n# Train the model\nepochs = 1000\nlosses = []\n\nfor epoch in range(epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train)\n    loss = criterion(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n\n    if (epoch+1) % 100 == 0:\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n\n    losses.append(loss.item())\n\nEpoch [100/1000], Loss: 0.5367\nEpoch [200/1000], Loss: 0.3958\nEpoch [300/1000], Loss: 0.3067\nEpoch [400/1000], Loss: 0.2522\nEpoch [500/1000], Loss: 0.2202\nEpoch [600/1000], Loss: 0.2062\nEpoch [700/1000], Loss: 0.1922\nEpoch [800/1000], Loss: 0.1563\nEpoch [900/1000], Loss: 0.0866\nEpoch [1000/1000], Loss: 0.0304\n\n\n\n# Plot the training loss\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.show()\n\n\n\n\n\n\n\n\n\nmodel.eval()\nwith torch.no_grad():\n    train_outputs = model(X_train)\n    test_outputs = model(X_test)\n    train_pred = torch.round(torch.sigmoid(train_outputs))\n    test_pred = torch.round(torch.sigmoid(test_outputs))\n\n    train_accuracy = (train_pred.eq(y_train).sum() / float(y_train.shape[0])).item()\n    test_accuracy = (test_pred.eq(y_test).sum() / float(y_test.shape[0])).item()\n\nprint(f'Train Accuracy: {train_accuracy*100:.2f}%')\nprint(f'Test Accuracy: {test_accuracy*100:.2f}%')\n\n# Plot the results\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.title('Train Data')\nplt.scatter(X_train[:, 0], X_train[:, 1], c=train_pred[:, 0], cmap='coolwarm', alpha=0.6)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.subplot(1, 2, 2)\nplt.title('Test Data')\nplt.scatter(X_test[:, 0], X_test[:, 1], c=test_pred[:, 0], cmap='coolwarm', alpha=0.6)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.show()\n\nTrain Accuracy: 99.87%\nTest Accuracy: 100.00%\n\n\n\n\n\n\n\n\n\n\n# Function 1: plot decision boundary\nimport numpy as np\ndef plot_decision_boundary(model: torch.nn.Module, X: torch.Tensor, y: torch.Tensor):\n    \"\"\"\n    Plots decision boundaries of model predictions on X in comparison to y.\n    \"\"\"\n    # Move model and data to CPU\n    model.to(\"cpu\")\n    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n\n    # Setup grid for plotting decision boundaries\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n\n    # Prepare data for prediction\n    X_to_pred_on = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()\n\n    # Make predictions\n    model.eval()\n    with torch.no_grad():\n        y_logits = model(X_to_pred_on)\n\n    # Adjust logits to prediction labels\n    if len(torch.unique(y)) &gt; 2:\n        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)  # multi-class\n    else:\n        y_pred = torch.round(torch.sigmoid(y_logits))  # binary\n\n    # Reshape predictions and plot\n    y_pred = y_pred.reshape(xx.shape).numpy()\n    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.show()\n\n# Function 2: plot predictions\ndef plot_predictions(train_data, train_labels, test_data, test_labels, predictions=None):\n    \"\"\"\n    Plots training and test data and compares predictions if provided.\n    \"\"\"\n    #plt.figure(figsize=(10, 7))\n\n    # Plot training data\n    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n\n    # Plot test data\n    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n\n    if predictions is not None:\n        # Plot predictions on test data\n        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n\n    # Display legend\n    plt.legend(prop={\"size\": 14})\n    plt.show()\n\n\n# Plot decision boundaries for training and test sets\nplt.figure(figsize=(6, 6))\nplt.title(\"Train\")\nplot_decision_boundary(model, X_train, y_train)\n\nplt.figure(figsize=(6, 6))\nplt.title(\"Test\")\nplot_decision_boundary(model, X_test, y_test)"
  },
  {
    "objectID": "Basic_DL/Basic_DL/Handon1.html",
    "href": "Basic_DL/Basic_DL/Handon1.html",
    "title": "basic-DL",
    "section": "",
    "text": "The process of creating a neural network using PyTorch with three hidden layers to fit a complex function. We’ll use a synthetic dataset for demonstration purposes. Here’s a step-by-step approach:\n\nImport Libraries: First, we need to import the necessary libraries.\n\n\n### Step 1: Import Libraries\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nCreate Synthetic Data: Generate some synthetic data that represents a complex function. For this example, we’ll use a sine wave with added noise.\n\n\n### Step 2: Create Synthetic Data\n\n# Generate synthetic data\nnp.random.seed(42)\nx = np.linspace(-2 * np.pi, 2 * np.pi, 1000)\ny = np.sin(x) + 0.1 * np.random.normal(size=x.shape)\n\n# Convert to PyTorch tensors\nx_tensor = torch.tensor(x, dtype=torch.float32).view(-1, 1)\ny_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n\n\nplt.plot(x,y)\n\n\n\n\n\n\n\n\n\nDefine the Neural Network: Create a neural network with three hidden layers.\n\n\n### Step 3: Define the Neural Network\nrelu=True #none-linear activation function\n#relu=False #none-linear activation function\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.layer1 = nn.Linear(1, 64)\n        self.layer2 = nn.Linear(64, 128)\n        self.layer3 = nn.Linear(128, 64)\n        self.layer4 = nn.Linear(64, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n      if relu:\n        x = self.relu(self.layer1(x))\n        x = self.relu(self.layer2(x))\n        x = self.relu(self.layer3(x))\n        x = self.layer4(x)\n        return x\n      else:\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\nmodel = NeuralNetwork()\n\n\nDefine Loss and Optimizer: Choose a loss function and an optimizer.\n\n\n### Step 4: Define Loss and Optimizer\n\ncriterion = nn.MSELoss() #Mean Square Error(MSE) or L2 loss\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n#Adam (Adaptive Moment Estimation) optimizer computes adaptive learning rates for each parameter.\n#lr=0.001: This sets the learning rate for the optimizer\n\n\nTrain the Model: Train the neural network on the synthetic data.\n\n\n### Step 5: Train the Model\n\nnum_epochs = 1000\nlosses = []\n\nfor epoch in range(num_epochs):\n    model.train() #Sets the model to training mode\n    optimizer.zero_grad() #Clears old gradients to prevent accumulation from previous iterations.\n    outputs = model(x_tensor) #Performs a forward pass, generating predictions from the input data.\n    loss = criterion(outputs, y_tensor) # Calculates the loss (error) between the predictions and true targets.\n    loss.backward()  #Computes gradients of the loss with respect to the model parameters (backpropagation).\n    optimizer.step() #Updates the model parameters using the computed gradients to minimize the loss.\n\n    if (epoch+1) % 100 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n    losses.append(loss.item())\n\nEpoch [100/1000], Loss: 0.0630\nEpoch [200/1000], Loss: 0.0122\nEpoch [300/1000], Loss: 0.0106\nEpoch [400/1000], Loss: 0.0098\nEpoch [500/1000], Loss: 0.0098\nEpoch [600/1000], Loss: 0.0094\nEpoch [700/1000], Loss: 0.0095\nEpoch [800/1000], Loss: 0.0095\nEpoch [900/1000], Loss: 0.0101\nEpoch [1000/1000], Loss: 0.0092\n\n\n\n# Plot the training loss\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.show()\n\n\n\n\n\n\n\n\n\nEvaluate the Model: Evaluate the model’s performance.\n\n\n### Step 6: Evaluate the Model\n\nmodel.eval()\nwith torch.no_grad():\n    predicted = model(x_tensor).numpy()\n\n# Plot the results\nplt.plot(x, y, label='Original Data')\nplt.plot(x, predicted, label='Fitted Data')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "Basic_DL/Basic_DL/Handon4.html",
    "href": "Basic_DL/Basic_DL/Handon4.html",
    "title": "basic-DL",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Define a transform to normalize the data\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\n# Load the training and test datasets\ntrainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n\ntestset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n\n\ndef imshow(img):\n    img = img / 2 + 0.5  # Unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\n# Get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# Plot 20 images in a 4x5 grid\nfig, axes = plt.subplots(nrows=4, ncols=5, figsize=(10, 10))\naxes = axes.flatten()\n\nfor idx, ax in enumerate(axes):\n    if idx &lt; len(images):\n        ax.imshow(images[idx].numpy().squeeze(), cmap='gray')\n        ax.set_title(f'Label: {labels[idx].item()}')\n        ax.axis('off')\n    else:\n        ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Define the Neural Network\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(28*28, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 128)\n        self.fc4 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 28*28)  # Flatten the image\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = torch.relu(self.fc3(x))\n        x = self.fc4(x)\n        return x\n\nnet = Net()\n\n\n# Define the Loss Function and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(), lr=0.001)\n\n\n# Lists to keep track of losses\ntrain_losses = []\ntest_losses = []\n\nfor epoch in range(10):  # Loop over the dataset multiple times\n    running_loss = 0.0\n    net.train()  # Set the network to training mode\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        # Print statistics\n        running_loss += loss.item()\n        if i % 100 == 99:    # Print every 100 mini-batches\n            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n            running_loss = 0.0\n\n    # Track training loss\n    train_losses.append(loss.item())\n\n    # Track test loss\n    net.eval()  # Set the network to evaluation mode\n    test_loss = 0.0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            outputs = net(images)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n    test_losses.append(test_loss / len(testloader))\n\nprint('Finished Training')\n\n[Epoch 1, Batch 100] loss: 0.843\n[Epoch 1, Batch 200] loss: 0.407\n[Epoch 1, Batch 300] loss: 0.317\n[Epoch 1, Batch 400] loss: 0.297\n[Epoch 1, Batch 500] loss: 0.234\n[Epoch 1, Batch 600] loss: 0.228\n[Epoch 1, Batch 700] loss: 0.209\n[Epoch 1, Batch 800] loss: 0.199\n[Epoch 1, Batch 900] loss: 0.183\n[Epoch 2, Batch 100] loss: 0.162\n[Epoch 2, Batch 200] loss: 0.165\n[Epoch 2, Batch 300] loss: 0.144\n[Epoch 2, Batch 400] loss: 0.146\n[Epoch 2, Batch 500] loss: 0.142\n[Epoch 2, Batch 600] loss: 0.131\n[Epoch 2, Batch 700] loss: 0.144\n[Epoch 2, Batch 800] loss: 0.136\n[Epoch 2, Batch 900] loss: 0.140\n[Epoch 3, Batch 100] loss: 0.109\n[Epoch 3, Batch 200] loss: 0.100\n[Epoch 3, Batch 300] loss: 0.118\n[Epoch 3, Batch 400] loss: 0.099\n[Epoch 3, Batch 500] loss: 0.109\n[Epoch 3, Batch 600] loss: 0.103\n[Epoch 3, Batch 700] loss: 0.107\n[Epoch 3, Batch 800] loss: 0.105\n[Epoch 3, Batch 900] loss: 0.097\n[Epoch 4, Batch 100] loss: 0.088\n[Epoch 4, Batch 200] loss: 0.073\n[Epoch 4, Batch 300] loss: 0.094\n[Epoch 4, Batch 400] loss: 0.092\n[Epoch 4, Batch 500] loss: 0.091\n[Epoch 4, Batch 600] loss: 0.089\n[Epoch 4, Batch 700] loss: 0.086\n[Epoch 4, Batch 800] loss: 0.095\n[Epoch 4, Batch 900] loss: 0.092\n[Epoch 5, Batch 100] loss: 0.075\n[Epoch 5, Batch 200] loss: 0.067\n[Epoch 5, Batch 300] loss: 0.073\n[Epoch 5, Batch 400] loss: 0.081\n[Epoch 5, Batch 500] loss: 0.062\n[Epoch 5, Batch 600] loss: 0.079\n[Epoch 5, Batch 700] loss: 0.069\n[Epoch 5, Batch 800] loss: 0.081\n[Epoch 5, Batch 900] loss: 0.082\n[Epoch 6, Batch 100] loss: 0.066\n[Epoch 6, Batch 200] loss: 0.067\n[Epoch 6, Batch 300] loss: 0.060\n[Epoch 6, Batch 400] loss: 0.064\n[Epoch 6, Batch 500] loss: 0.070\n[Epoch 6, Batch 600] loss: 0.055\n[Epoch 6, Batch 700] loss: 0.083\n[Epoch 6, Batch 800] loss: 0.061\n[Epoch 6, Batch 900] loss: 0.076\n[Epoch 7, Batch 100] loss: 0.052\n[Epoch 7, Batch 200] loss: 0.058\n[Epoch 7, Batch 300] loss: 0.052\n[Epoch 7, Batch 400] loss: 0.062\n[Epoch 7, Batch 500] loss: 0.060\n[Epoch 7, Batch 600] loss: 0.062\n[Epoch 7, Batch 700] loss: 0.067\n[Epoch 7, Batch 800] loss: 0.063\n[Epoch 7, Batch 900] loss: 0.059\n[Epoch 8, Batch 100] loss: 0.045\n[Epoch 8, Batch 200] loss: 0.046\n[Epoch 8, Batch 300] loss: 0.051\n[Epoch 8, Batch 400] loss: 0.053\n[Epoch 8, Batch 500] loss: 0.057\n[Epoch 8, Batch 600] loss: 0.052\n[Epoch 8, Batch 700] loss: 0.052\n[Epoch 8, Batch 800] loss: 0.051\n[Epoch 8, Batch 900] loss: 0.043\n[Epoch 9, Batch 100] loss: 0.043\n[Epoch 9, Batch 200] loss: 0.050\n[Epoch 9, Batch 300] loss: 0.040\n[Epoch 9, Batch 400] loss: 0.047\n[Epoch 9, Batch 500] loss: 0.049\n[Epoch 9, Batch 600] loss: 0.060\n[Epoch 9, Batch 700] loss: 0.040\n[Epoch 9, Batch 800] loss: 0.048\n[Epoch 9, Batch 900] loss: 0.049\n[Epoch 10, Batch 100] loss: 0.034\n[Epoch 10, Batch 200] loss: 0.045\n[Epoch 10, Batch 300] loss: 0.048\n[Epoch 10, Batch 400] loss: 0.045\n[Epoch 10, Batch 500] loss: 0.045\n[Epoch 10, Batch 600] loss: 0.043\n[Epoch 10, Batch 700] loss: 0.050\n[Epoch 10, Batch 800] loss: 0.045\n[Epoch 10, Batch 900] loss: 0.055\nFinished Training\n\n\n\n# Plot the training and test losses\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Training Loss')\nplt.plot(test_losses, label='Test Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Test Loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Test the Network\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\n\nAccuracy of the network on the 10000 test images: 97.75%"
  },
  {
    "objectID": "Basic_DL/Basic_DL/index1.html",
    "href": "Basic_DL/Basic_DL/index1.html",
    "title": "Basic_DL",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Basic_DL/Basic_DL/Handon2.html",
    "href": "Basic_DL/Basic_DL/Handon2.html",
    "title": "basic-DL",
    "section": "",
    "text": "We’ll create a neural network with three hidden layers to fit the data. Here’s the step-by-step approach:\n# Step 1: Import Libraries\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_circles\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n# Step 2: Create the Dataset\n\n# Generate synthetic data\nX, y = make_circles(n_samples=1000, noise=0.03, random_state=42)\n\n# Standardize the data\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Convert to PyTorch tensors\nX_tensor = torch.tensor(X, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n\n# Split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n# Visualize with a plot\nimport matplotlib.pyplot as plt\nplt.scatter(x=X[:, 0],\n            y=X[:, 1],\n            c=y,\n            cmap=plt.cm.RdYlBu);"
  },
  {
    "objectID": "Basic_DL/Basic_DL/Handon2.html#the-architecture-of-the-neural-network",
    "href": "Basic_DL/Basic_DL/Handon2.html#the-architecture-of-the-neural-network",
    "title": "basic-DL",
    "section": "The architecture of the neural network",
    "text": "The architecture of the neural network\n\nInput Size = 2\nThe input size of 2 is determined by the nature of the dataset we’re working with. The make_circles function from sklearn.datasets generates a 2-dimensional dataset where each data point has two features (coordinates in a 2D space). Therefore, the input to the neural network has to accommodate these two features, which is why the first layer (nn.Linear(2, 64)) expects an input size of 2.\n\n\nOutput Size = 1\nThe output size of 1 is determined by the task we’re trying to solve, which is a binary classification problem. The make_circles dataset labels each data point as either 0 or 1. Thus, the neural network needs to output a single value for each data point that can be interpreted as the probability of the data point belonging to class 1. The final layer (nn.Linear(64, 1)) reduces the output to a single value for each input data point.\n\n\nNetwork Architecture\nHere’s a brief breakdown of the network layers:\n\nInput Layer: self.layer1 = nn.Linear(2, 64)\n\nTakes 2 input features (the 2D coordinates).\nOutputs 64 features.\nActivation function: ReLU (self.relu).\n\nFirst Hidden Layer: self.layer2 = nn.Linear(64, 128)\n\nTakes the 64 features from the previous layer.\nOutputs 128 features.\nActivation function: ReLU.\n\nSecond Hidden Layer: self.layer3 = nn.Linear(128, 64)\n\nTakes the 128 features from the previous layer.\nOutputs 64 features.\nActivation function: ReLU.\n\nOutput Layer: self.layer4 = nn.Linear(64, 1)\n\nTakes the 64 features from the previous layer.\nOutputs a single feature, representing the predicted probability of the input belonging to class 1.\n\n\n\n\nActivation Function: ReLU\nThe ReLU (Rectified Linear Unit) activation function is applied after each hidden layer to introduce non-linearity into the model, allowing it to learn more complex patterns.\n\n# Step 3: Define the Neural Network\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n        self.layer1 = nn.Linear(2, 64)\n        self.layer2 = nn.Linear(64, 128)\n        self.layer3 = nn.Linear(128, 64)\n        self.layer4 = nn.Linear(64, 1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.layer1(x))\n        x = self.relu(self.layer2(x))\n        x = self.relu(self.layer3(x))\n        x = self.layer4(x)\n        return x\n\nmodel = NeuralNetwork()\n\n\n# Step 4: Define Loss and Optimizer\n\ncriterion = nn.BCEWithLogitsLoss()\n#BCEWithLogitsLoss: Binary Cross Entropy with Logits Loss which is suitable for binary classification tasks\n    #•  Binary Cross Entropy (BCE) Loss: Measures the difference between true labels and predicted probabilities.\n    #•  With Logits: Expects raw output (logits) from the model, not probabilities. Internally applies the sigmoid function to logits before computing the loss.\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n#Adam (Adaptive Moment Estimation) optimizer computes adaptive learning rates for each parameter.\n#lr=0.001: This sets the learning rate for the optimizer\n\n\n# Step 5: Train the Model\n\nnum_epochs = 1000\nlosses = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(X_train)\n    loss = criterion(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n\n    if (epoch+1) % 100 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n    losses.append(loss.item())\n\nEpoch [100/1000], Loss: 0.0097\nEpoch [200/1000], Loss: 0.0017\nEpoch [300/1000], Loss: 0.0007\nEpoch [400/1000], Loss: 0.0003\nEpoch [500/1000], Loss: 0.0002\nEpoch [600/1000], Loss: 0.0001\nEpoch [700/1000], Loss: 0.0001\nEpoch [800/1000], Loss: 0.0001\nEpoch [900/1000], Loss: 0.0001\nEpoch [1000/1000], Loss: 0.0000\n\n\n\n# Plot the training loss\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.show()\n\n\n\n\n\n\n\n\n\nmodel.eval()\nwith torch.no_grad():\n    train_outputs = model(X_train)\n    test_outputs = model(X_test)\n    train_pred = torch.round(torch.sigmoid(train_outputs))\n    test_pred = torch.round(torch.sigmoid(test_outputs))\n\n    train_accuracy = (train_pred.eq(y_train).sum() / float(y_train.shape[0])).item()\n    test_accuracy = (test_pred.eq(y_test).sum() / float(y_test.shape[0])).item()\n\nprint(f'Train Accuracy: {train_accuracy*100:.2f}%')\nprint(f'Test Accuracy: {test_accuracy*100:.2f}%')\n\n# Plot the results\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.title('Train Data')\nplt.scatter(X_train[:, 0], X_train[:, 1], c=train_pred[:, 0], cmap='coolwarm', alpha=0.6)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.subplot(1, 2, 2)\nplt.title('Test Data')\nplt.scatter(X_test[:, 0], X_test[:, 1], c=test_pred[:, 0], cmap='coolwarm', alpha=0.6)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.show()\n\nTrain Accuracy: 100.00%\nTest Accuracy: 100.00%\n\n\n\n\n\n\n\n\n\n\n# Function 1: plot decision boundary\n\ndef plot_decision_boundary(model: torch.nn.Module, X: torch.Tensor, y: torch.Tensor):\n    \"\"\"\n    Plots decision boundaries of model predictions on X in comparison to y.\n    \"\"\"\n    # Move model and data to CPU\n    model.to(\"cpu\")\n    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n\n    # Setup grid for plotting decision boundaries\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n\n    # Prepare data for prediction\n    X_to_pred_on = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()\n\n    # Make predictions\n    model.eval()\n    with torch.no_grad():\n        y_logits = model(X_to_pred_on)\n\n    # Adjust logits to prediction labels\n    if len(torch.unique(y)) &gt; 2:\n        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)  # multi-class\n    else:\n        y_pred = torch.round(torch.sigmoid(y_logits))  # binary\n\n    # Reshape predictions and plot\n    y_pred = y_pred.reshape(xx.shape).numpy()\n    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.show()\n\n# Function 2: plot predictions\ndef plot_predictions(train_data, train_labels, test_data, test_labels, predictions=None):\n    \"\"\"\n    Plots training and test data and compares predictions if provided.\n    \"\"\"\n    plt.figure(figsize=(10, 7))\n\n    # Plot training data\n    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n\n    # Plot test data\n    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n\n    if predictions is not None:\n        # Plot predictions on test data\n        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n\n    # Display legend\n    plt.legend(prop={\"size\": 14})\n    plt.show()\n\n\n# Plot decision boundaries for training and test sets\nplt.figure(figsize=(6, 6))\nplt.title(\"Train\")\nplot_decision_boundary(model, X_train, y_train)\n\nplt.figure(figsize=(6, 6))\nplt.title(\"Test\")\nplot_decision_boundary(model, X_test, y_test)"
  },
  {
    "objectID": "Handon4.html",
    "href": "Handon4.html",
    "title": "basic-DL",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Define a transform to normalize the data\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\n# Load the training and test datasets\ntrainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n\ntestset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n\n\ndef imshow(img):\n    img = img / 2 + 0.5  # Unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\n# Get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# Plot 20 images in a 4x5 grid\nfig, axes = plt.subplots(nrows=4, ncols=5, figsize=(10, 10))\naxes = axes.flatten()\n\nfor idx, ax in enumerate(axes):\n    if idx &lt; len(images):\n        ax.imshow(images[idx].numpy().squeeze(), cmap='gray')\n        ax.set_title(f'Label: {labels[idx].item()}')\n        ax.axis('off')\n    else:\n        ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Define the Neural Network\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(28*28, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 128)\n        self.fc4 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = x.view(-1, 28*28)  # Flatten the image\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = torch.relu(self.fc3(x))\n        x = self.fc4(x)\n        return x\n\nnet = Net()\n\n\n# Define the Loss Function and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(), lr=0.001)\n\n\n# Lists to keep track of losses\ntrain_losses = []\ntest_losses = []\n\nfor epoch in range(10):  # Loop over the dataset multiple times\n    running_loss = 0.0\n    net.train()  # Set the network to training mode\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        # Print statistics\n        running_loss += loss.item()\n        if i % 100 == 99:    # Print every 100 mini-batches\n            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n            running_loss = 0.0\n\n    # Track training loss\n    train_losses.append(loss.item())\n\n    # Track test loss\n    net.eval()  # Set the network to evaluation mode\n    test_loss = 0.0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            outputs = net(images)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n    test_losses.append(test_loss / len(testloader))\n\nprint('Finished Training')\n\n[Epoch 1, Batch 100] loss: 0.888\n[Epoch 1, Batch 200] loss: 0.400\n[Epoch 1, Batch 300] loss: 0.314\n[Epoch 1, Batch 400] loss: 0.273\n[Epoch 1, Batch 500] loss: 0.244\n[Epoch 1, Batch 600] loss: 0.242\n[Epoch 1, Batch 700] loss: 0.214\n[Epoch 1, Batch 800] loss: 0.189\n[Epoch 1, Batch 900] loss: 0.199\n[Epoch 2, Batch 100] loss: 0.150\n[Epoch 2, Batch 200] loss: 0.156\n[Epoch 2, Batch 300] loss: 0.158\n[Epoch 2, Batch 400] loss: 0.141\n[Epoch 2, Batch 500] loss: 0.146\n[Epoch 2, Batch 600] loss: 0.133\n[Epoch 2, Batch 700] loss: 0.137\n[Epoch 2, Batch 800] loss: 0.134\n[Epoch 2, Batch 900] loss: 0.134\n[Epoch 3, Batch 100] loss: 0.110\n[Epoch 3, Batch 200] loss: 0.113\n[Epoch 3, Batch 300] loss: 0.113\n[Epoch 3, Batch 400] loss: 0.109\n[Epoch 3, Batch 500] loss: 0.110\n[Epoch 3, Batch 600] loss: 0.097\n[Epoch 3, Batch 700] loss: 0.115\n[Epoch 3, Batch 800] loss: 0.120\n[Epoch 3, Batch 900] loss: 0.090\n[Epoch 4, Batch 100] loss: 0.087\n[Epoch 4, Batch 200] loss: 0.094\n[Epoch 4, Batch 300] loss: 0.081\n[Epoch 4, Batch 400] loss: 0.084\n[Epoch 4, Batch 500] loss: 0.085\n[Epoch 4, Batch 600] loss: 0.088\n[Epoch 4, Batch 700] loss: 0.083\n[Epoch 4, Batch 800] loss: 0.094\n[Epoch 4, Batch 900] loss: 0.099\n[Epoch 5, Batch 100] loss: 0.097\n[Epoch 5, Batch 200] loss: 0.060\n[Epoch 5, Batch 300] loss: 0.069\n[Epoch 5, Batch 400] loss: 0.074\n[Epoch 5, Batch 500] loss: 0.087\n[Epoch 5, Batch 600] loss: 0.079\n[Epoch 5, Batch 700] loss: 0.074\n[Epoch 5, Batch 800] loss: 0.075\n[Epoch 5, Batch 900] loss: 0.084\n[Epoch 6, Batch 100] loss: 0.069\n[Epoch 6, Batch 200] loss: 0.065\n[Epoch 6, Batch 300] loss: 0.064\n[Epoch 6, Batch 400] loss: 0.075\n[Epoch 6, Batch 500] loss: 0.068\n[Epoch 6, Batch 600] loss: 0.062\n[Epoch 6, Batch 700] loss: 0.070\n[Epoch 6, Batch 800] loss: 0.065\n[Epoch 6, Batch 900] loss: 0.077\n[Epoch 7, Batch 100] loss: 0.046\n[Epoch 7, Batch 200] loss: 0.063\n[Epoch 7, Batch 300] loss: 0.066\n[Epoch 7, Batch 400] loss: 0.052\n[Epoch 7, Batch 500] loss: 0.058\n[Epoch 7, Batch 600] loss: 0.056\n[Epoch 7, Batch 700] loss: 0.056\n[Epoch 7, Batch 800] loss: 0.075\n[Epoch 7, Batch 900] loss: 0.063\n[Epoch 8, Batch 100] loss: 0.042\n[Epoch 8, Batch 200] loss: 0.069\n[Epoch 8, Batch 300] loss: 0.041\n[Epoch 8, Batch 400] loss: 0.050\n[Epoch 8, Batch 500] loss: 0.053\n[Epoch 8, Batch 600] loss: 0.052\n[Epoch 8, Batch 700] loss: 0.049\n[Epoch 8, Batch 800] loss: 0.049\n[Epoch 8, Batch 900] loss: 0.052\n[Epoch 9, Batch 100] loss: 0.046\n[Epoch 9, Batch 200] loss: 0.045\n[Epoch 9, Batch 300] loss: 0.047\n[Epoch 9, Batch 400] loss: 0.041\n[Epoch 9, Batch 500] loss: 0.047\n[Epoch 9, Batch 600] loss: 0.047\n[Epoch 9, Batch 700] loss: 0.058\n[Epoch 9, Batch 800] loss: 0.063\n[Epoch 9, Batch 900] loss: 0.053\n[Epoch 10, Batch 100] loss: 0.033\n[Epoch 10, Batch 200] loss: 0.038\n[Epoch 10, Batch 300] loss: 0.039\n[Epoch 10, Batch 400] loss: 0.039\n[Epoch 10, Batch 500] loss: 0.048\n[Epoch 10, Batch 600] loss: 0.044\n[Epoch 10, Batch 700] loss: 0.049\n[Epoch 10, Batch 800] loss: 0.039\n[Epoch 10, Batch 900] loss: 0.060\nFinished Training\n\n\n\n# Plot the training and test losses\nplt.figure(figsize=(10, 5))\nplt.plot(train_losses, label='Training Loss')\nplt.plot(test_losses, label='Test Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Test Loss')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Test the Network\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')\n\nAccuracy of the network on the 10000 test images: 97.43%"
  },
  {
    "objectID": "Handon3.html",
    "href": "Handon3.html",
    "title": "basic-DL",
    "section": "",
    "text": "Google colab - Hand on 3\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport warnings\n# Generate the dataset\nX, y = make_moons(n_samples=1000, noise=0.03, random_state=42)\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Convert the data to PyTorch tensors\nX_train = torch.tensor(X_train, dtype=torch.float32)\nX_test = torch.tensor(X_test, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Change to (N, 1)\ny_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)  # Change to (N, 1)\n#- Define the neural network model\nclass MoonModel(nn.Module):\n    def __init__(self):\n        super(MoonModel, self).__init__()\n        # Define neral network layers:\n        #TODO\n\n\n    # Define forward propagation function\n    def forward(self, x):\n        #TODO\n\n\nmodel = MoonModel()\n# Define the loss function and the optimizer\ncriterion = #TODO\noptimizer = #TODO\n#- Train the model\nepochs =  #TODO # Set the number of epochs\nlosses = []\n\nfor epoch in range(epochs):\n\n    # Set model into  training mode\n      #TODO\n\n    # Clear loss gradients from the previous epoch\n      #TODO\n\n    # Get outputs from the current model and training data\n      #TODO\n\n    # Estimate loss\n      #TODO\n\n    # Calculate loss gradient\n      #TODO\n\n    # Update model parameteres using the computed loss gradient\n      #TODO\n\n    if (epoch+1) % 100 == 0:\n        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n\n    losses.append(loss.item())\n# Plot the training loss\nplt.plot(losses)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.show()\nmodel.eval()\nwith torch.no_grad():\n    train_outputs = model(X_train)\n    test_outputs = model(X_test)\n    train_pred = torch.round(torch.sigmoid(train_outputs))\n    test_pred = torch.round(torch.sigmoid(test_outputs))\n\n    train_accuracy = (train_pred.eq(y_train).sum() / float(y_train.shape[0])).item()\n    test_accuracy = (test_pred.eq(y_test).sum() / float(y_test.shape[0])).item()\n\nprint(f'Train Accuracy: {train_accuracy*100:.2f}%')\nprint(f'Test Accuracy: {test_accuracy*100:.2f}%')\n\n# Plot the results\nplt.figure(figsize=(10, 5))\n\nplt.subplot(1, 2, 1)\nplt.title('Train Data')\nplt.scatter(X_train[:, 0], X_train[:, 1], c=train_pred[:, 0], cmap='coolwarm', alpha=0.6)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.subplot(1, 2, 2)\nplt.title('Test Data')\nplt.scatter(X_test[:, 0], X_test[:, 1], c=test_pred[:, 0], cmap='coolwarm', alpha=0.6)\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\n\nplt.show()\n# Function 1: plot decision boundary\nimport numpy as np\ndef plot_decision_boundary(model: torch.nn.Module, X: torch.Tensor, y: torch.Tensor):\n    \"\"\"\n    Plots decision boundaries of model predictions on X in comparison to y.\n    \"\"\"\n    # Move model and data to CPU\n    model.to(\"cpu\")\n    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n\n    # Setup grid for plotting decision boundaries\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n\n    # Prepare data for prediction\n    X_to_pred_on = torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()\n\n    # Make predictions\n    model.eval()\n    with torch.no_grad():\n        y_logits = model(X_to_pred_on)\n\n    # Adjust logits to prediction labels\n    if len(torch.unique(y)) &gt; 2:\n        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)  # multi-class\n    else:\n        y_pred = torch.round(torch.sigmoid(y_logits))  # binary\n\n    # Reshape predictions and plot\n    y_pred = y_pred.reshape(xx.shape).numpy()\n    plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.show()\n\n# Function 2: plot predictions\ndef plot_predictions(train_data, train_labels, test_data, test_labels, predictions=None):\n    \"\"\"\n    Plots training and test data and compares predictions if provided.\n    \"\"\"\n    #plt.figure(figsize=(10, 7))\n\n    # Plot training data\n    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n\n    # Plot test data\n    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n\n    if predictions is not None:\n        # Plot predictions on test data\n        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n\n    # Display legend\n    plt.legend(prop={\"size\": 14})\n    plt.show()\n# Plot decision boundaries for training and test sets\nplt.figure(figsize=(6, 6))\nplt.title(\"Train\")\nplot_decision_boundary(model, X_train, y_train)\n\nplt.figure(figsize=(6, 6))\nplt.title(\"Test\")\nplot_decision_boundary(model, X_test, y_test)"
  },
  {
    "objectID": "DL/DL/intro_pytorch.html",
    "href": "DL/DL/intro_pytorch.html",
    "title": "basic-DL",
    "section": "",
    "text": "Neural networks are computational models inspired by the human brain. They consist of layers of interconnected nodes (neurons) where each connection has an associated weight. The basic flow in a neural network involves:\n\nForward Propagation: Input data passes through the network layers to produce an output.\nLoss Calculation: The output is compared with the actual target to compute the loss.\nBackpropagation: The network adjusts the weights to minimize the loss."
  },
  {
    "objectID": "DL/DL/intro_pytorch.html#introduction-to-neural-networks-with-pytorch",
    "href": "DL/DL/intro_pytorch.html#introduction-to-neural-networks-with-pytorch",
    "title": "basic-DL",
    "section": "Introduction to Neural Networks with PyTorch",
    "text": "Introduction to Neural Networks with PyTorch\nPyTorch is a popular open-source deep learning framework that offers a flexible and efficient platform for building and training neural networks. This guide introduces the basic concepts of neural networks, including forward propagation, different layers, activation functions, backpropagation, and loss functions, and demonstrates how to implement these concepts using PyTorch.\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\n\n# Define a neural network with two hidden layers\nclass SimpleNN(nn.Module):\n    def __init__(self):\n        super(SimpleNN, self).__init__()\n        self.fc1 = nn.Linear(10, 50)  # First hidden layer\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(50, 50)  # Second hidden layer\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(50, 1)   # Output layer\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu1(x)\n        x = self.fc2(x)\n        x = self.relu2(x)\n        x = self.fc3(x)\n        return x\n\n# Instantiate the model\nmodel = SimpleNN()\n\n# Generate random input data and targets\ninputs = [torch.randn(10) for _ in range(1000)]\ntargets = [torch.tensor([1.0]) for _ in range(1000)]  # Example target\n\n# Split data into training and test sets\ntrain_inputs, test_inputs, train_targets, test_targets = train_test_split(inputs, targets, test_size=0.2)\n\n# Define a loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Number of epochs\nnum_epochs = 10\n\n# Training loop\nfor epoch in range(num_epochs):\n    total_loss = 0\n    for input, target in zip(train_inputs, train_targets):\n        # Forward pass\n        output = model(input)\n        loss = criterion(output, target)\n        \n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    # Print average training loss\n    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {total_loss/len(train_inputs)}')\n\n# Evaluate on test set\ntest_loss = 0\nwith torch.no_grad():  # No need to compute gradients for testing\n    for input, target in zip(test_inputs, test_targets):\n        output = model(input)\n        loss = criterion(output, target)\n        test_loss += loss.item()\n\n# Print average test loss\nprint(f'Average Test Loss: {test_loss/len(test_inputs)}')\n\nLoss Function and Optimization\nIn machine learning, the loss function (or cost function) measures how well the model’s predictions match the actual target values. The goal of training a neural network is to minimize this loss function.\n\n\nLocal Minimum vs. Global Minimum\n\nGlobal Minimum: This is the lowest possible point of the loss function. If we reach this point, the model parameters are optimal, and the model performs best on the training data.\nLocal Minimum: These are points where the loss function has a lower value than in its immediate vicinity, but not the lowest possible value. There can be multiple local minima.\n\n\n\nChallenges with Local Minima\n\nTrapped in Local Minima: During training, the optimization algorithm might get trapped in a local minimum, preventing the model from reaching the global minimum.\nPoor Generalization: Models trained in local minima might not generalize well to unseen data.\n\n\n\nWhy Use the Adam Optimizer?\n\nThe Adam (Adaptive Moment Estimation) optimizer is an advanced optimization algorithm that combines the benefits of two other extensions of stochastic gradient descent: Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). Here’s why Adam is beneficial:\n\nAdaptive Learning Rates: Adam adjusts the learning rate for each parameter dynamically based on estimates of lower-order moments. This allows for efficient and effective training.\nMomentum: Adam incorporates the concept of momentum, which helps to smooth out the update process by considering the past gradients, thus potentially avoiding local minima.\nBias Correction: Adam includes bias-correction mechanisms to ensure that the moment estimates are unbiased, particularly during the initial steps of training.\nComputational Efficiency: Adam is computationally efficient and has low memory requirements, making it suitable for large-scale data and models.\nRobustness: Adam works well with noisy data and sparse gradients, which are common in practical deep learning applications.\n\n\n\nGeneral Structure of Constructing and Training a Deep Learning Model\nHere are the general steps to build and train a deep learning model:\n\nDefine the Model Architecture\n\nSpecify the type of model (e.g., feedforward neural network, convolutional neural network).\nDefine the number of layers and the type of each layer (e.g., fully connected, convolutional, dropout).\nChoose activation functions for each layer (e.g., ReLU, sigmoid).\n\nPrepare the Data\n\nCollect and preprocess the data (e.g., normalization, resizing, augmentation).\nSplit the data into training, validation, and test sets.\n\nDefine the Loss Function and Optimizer\n\nChoose a loss function appropriate for the task (e.g., cross-entropy for classification, mean squared error for regression).\nSelect an optimization algorithm (e.g., SGD, Adam).\n\nImplement the Training Loop\n\nInitialize model parameters.\nFor a specified number of epochs:\n\nForward Pass: Pass the input data through the model to get the output.\nCompute Loss: Calculate the loss using the model’s output and the actual target values.\nBackward Pass: Compute gradients by backpropagation.\nUpdate Parameters: Update the model’s parameters using the optimizer.\nOptionally, compute and log metrics on the validation set to monitor training progress.\n\n\nEvaluate the Model\n\nAfter training, evaluate the model on the test set to assess its performance.\n\nFine-tune and Deploy\n\nFine-tune the model by adjusting hyperparameters, adding regularization, or using more advanced architectures.\nOnce satisfied with the performance, deploy the model for inference on new data."
  },
  {
    "objectID": "DL/DL/index.html",
    "href": "DL/DL/index.html",
    "title": "basic-DL",
    "section": "",
    "text": "Machine learning (ML) is a subset of artificial intelligence (AI) that enables machines to learn from experience and act automatically. ML algorithms can be broadly classified into three categories: supervised learning, unsupervised learning, and reinforcement learning.\n\n\nSupervised learning involves training ML algorithms on labeled datasets where both input and output parameters are known. The goal is to develop a regression or classification model that can be used on new datasets to generate predictions or draw conclusions. Supervised learning algorithms learn to map input features to output labels, allowing them to make predictions or classifications on new, unseen data.\nExamples of supervised learning algorithms include:\n\nLinear Regression: A simple algorithm used to map the linear relationship between input features and a continuous target variable.\nLogistic Regression: An extension of linear regression used for classification tasks to estimate the likelihood that an instance belongs to a specific class.\nSupport Vector Machines (SVMs): Supervised learning algorithms that can perform classification and regression tasks by finding a hyperplane that best separates classes in feature space.\nK-Nearest Neighbors (KNN): A non-parametric technique used for classification as well as regression by identifying the k most similar data points to a new data point and predicting the label of the new data point using the labels of those data points.\nDecision Trees: A type of supervised learning technique used for classification as well as regression by segmenting the data into smaller and smaller groups until each group can be classified or predicted with high degree of accuracy.\nRandom Forest: An ensemble learning method that employs a set of decision trees to make predictions by aggregating predictions from individual trees.\nNaive Bayes: A probabilistic classifier based on Bayes’ theorem used for classification tasks by assuming that the features of a data point are independent of each other.\n\n\n\n\nUnsupervised learning involves training ML algorithms on unlabeled datasets where the data has yet to be categorized or labeled. The ML system itself learns to classify and process unlabeled data to learn from its inherent structure.\nExamples of unsupervised learning algorithms include:\n\nClustering Algorithms: Techniques used to group similar data points into clusters based on their features.\nAssociation Rule Learning Algorithms: Methods used to discover patterns and relationships between variables in the data.\nPrincipal Component Analysis (PCA): A dimensionality reduction technique used to transform data into a lower-dimensional space while retaining as much variance as possible.\n\n\n\n\nReinforcement learning involves training ML algorithms through ongoing interactions between an AI system and its environment. Algorithms receive numerical scores as rewards for generating decisions and outcomes, reinforcing positive interactions and behaviors over time.\nExamples of reinforcement learning algorithms include:\n\nMarkov Decision Processes (MDPs): Model-based methods used to solve sequential decision-making problems.\nQ-Learning: A model-free method used to learn the value function of an action in a given state.\nSARSA: Another model-free method used to learn the value function of an action in a given state.\nREINFORCE Algorithm: A policy-based method used to learn the policy of an agent in a given environment.\nActor-Critic Algorithm: A hybrid method used to learn both the policy and the value function of an agent in a given environment."
  },
  {
    "objectID": "DL/DL/index.html#machine-learning-ml",
    "href": "DL/DL/index.html#machine-learning-ml",
    "title": "basic-DL",
    "section": "",
    "text": "Machine learning (ML) is a subset of artificial intelligence (AI) that enables machines to learn from experience and act automatically. ML algorithms can be broadly classified into three categories: supervised learning, unsupervised learning, and reinforcement learning.\n\n\nSupervised learning involves training ML algorithms on labeled datasets where both input and output parameters are known. The goal is to develop a regression or classification model that can be used on new datasets to generate predictions or draw conclusions. Supervised learning algorithms learn to map input features to output labels, allowing them to make predictions or classifications on new, unseen data.\nExamples of supervised learning algorithms include:\n\nLinear Regression: A simple algorithm used to map the linear relationship between input features and a continuous target variable.\nLogistic Regression: An extension of linear regression used for classification tasks to estimate the likelihood that an instance belongs to a specific class.\nSupport Vector Machines (SVMs): Supervised learning algorithms that can perform classification and regression tasks by finding a hyperplane that best separates classes in feature space.\nK-Nearest Neighbors (KNN): A non-parametric technique used for classification as well as regression by identifying the k most similar data points to a new data point and predicting the label of the new data point using the labels of those data points.\nDecision Trees: A type of supervised learning technique used for classification as well as regression by segmenting the data into smaller and smaller groups until each group can be classified or predicted with high degree of accuracy.\nRandom Forest: An ensemble learning method that employs a set of decision trees to make predictions by aggregating predictions from individual trees.\nNaive Bayes: A probabilistic classifier based on Bayes’ theorem used for classification tasks by assuming that the features of a data point are independent of each other.\n\n\n\n\nUnsupervised learning involves training ML algorithms on unlabeled datasets where the data has yet to be categorized or labeled. The ML system itself learns to classify and process unlabeled data to learn from its inherent structure.\nExamples of unsupervised learning algorithms include:\n\nClustering Algorithms: Techniques used to group similar data points into clusters based on their features.\nAssociation Rule Learning Algorithms: Methods used to discover patterns and relationships between variables in the data.\nPrincipal Component Analysis (PCA): A dimensionality reduction technique used to transform data into a lower-dimensional space while retaining as much variance as possible.\n\n\n\n\nReinforcement learning involves training ML algorithms through ongoing interactions between an AI system and its environment. Algorithms receive numerical scores as rewards for generating decisions and outcomes, reinforcing positive interactions and behaviors over time.\nExamples of reinforcement learning algorithms include:\n\nMarkov Decision Processes (MDPs): Model-based methods used to solve sequential decision-making problems.\nQ-Learning: A model-free method used to learn the value function of an action in a given state.\nSARSA: Another model-free method used to learn the value function of an action in a given state.\nREINFORCE Algorithm: A policy-based method used to learn the policy of an agent in a given environment.\nActor-Critic Algorithm: A hybrid method used to learn both the policy and the value function of an agent in a given environment."
  },
  {
    "objectID": "DL/DL/index.html#introduction-deep-learning-dl",
    "href": "DL/DL/index.html#introduction-deep-learning-dl",
    "title": "basic-DL",
    "section": "Introduction Deep learning (DL)",
    "text": "Introduction Deep learning (DL)\nDeep learning (DL) is a powerful subset of machine learning techniques that are inspired by the structure and function of the brain’s neural networks. At the core of deep learning are artificial neural networks (ANNs), which are computational models composed of interconnected nodes called neurons. These neurons are organized into layers, allowing the network to learn hierarchical representations of data through a process called training.\nANNs are the fundamental building blocks of deep learning models. They consist of the following key components:\n\nLayers\n\nInput Layer: This layer receives the input data, such as an image or a text sequence.\nHidden Layers: These are the intermediate layers that transform the input data into increasingly abstract representations. The depth of a neural network refers to the number of hidden layers it has.\nOutput Layer: This layer produces the final output, such as a classification or a prediction.\n\n\n\nNeurons and Activation Functions\nEach layer in an ANN is composed of neurons, which are the basic computational units. Neurons receive input from the previous layer, perform a weighted sum of these inputs, and apply an activation function to produce an output that is passed to the next layer.\nCommon activation functions include:\n\nSigmoid: \\[\\sigma(x) = \\frac{1}{1 + e^{-x}}\\] [1]\nRectified Linear Unit (ReLU): \\[f(x) = \\max(0, x)\\] [1]\nHyperbolic Tangent (Tanh): \\[\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\] [1]\n\n\n\nTraining Neural Networks\nTraining an ANN involves the following key processes:\n\nForward Propagation: Input data is passed through the network, and the output is computed.\nLoss Function: A loss function measures the difference between the predicted output and the true target value. Common loss functions include Mean Squared Error (MSE) for regression tasks and Cross-Entropy Loss for classification tasks.\nBackpropagation: The gradients of the loss function with respect to the weights are computed using the chain rule of calculus. This process is called backpropagation.\nOptimization: The weights of the network are updated using an optimization algorithm, such as Gradient Descent, to minimize the loss function. Popular variants of Gradient Descent include Stochastic Gradient Descent (SGD), Mini-batch Gradient Descent, Adam, and RMSprop."
  },
  {
    "objectID": "DL/DL/index.html#applications-of-deep-learning",
    "href": "DL/DL/index.html#applications-of-deep-learning",
    "title": "basic-DL",
    "section": "Applications of Deep Learning",
    "text": "Applications of Deep Learning\nDeep learning has revolutionized various fields by providing state-of-the-art performance in numerous applications, including:\n\nComputer Vision: Image classification, object detection, image segmentation (e.g., using Convolutional Neural Networks (CNNs)).\nNatural Language Processing (NLP): Language translation, sentiment analysis, text generation (e.g., using Recurrent Neural Networks (RNNs) and Transformers).\nSpeech Recognition: Voice assistants, automated transcription.\nHealthcare: Disease diagnosis from medical images, personalized treatment recommendations.\nAutonomous Vehicles: Perception and decision-making systems.\nAstrophysics: Classifying celestial objects, detecting exoplanets, analyzing the interstellar medium.\n\n\nReferences\n\nSpringer, “Deep Learning: A Comprehensive Overview on Techniques and Applications” (2021)\nBiostrand AI, “AI, ML, DL, and NLP: An Overview” (2022)\nNCBI, “Introduction to Machine Learning, Neural Networks, and Deep Learning” (2020)\nClive Maxfield, “What the FAQ are AI, ANNs, ML, DL, and DNNs?” (n.d.)\nDataCamp, “Introduction to Deep Neural Networks” (n.d.)"
  },
  {
    "objectID": "about1.html",
    "href": "about1.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro_pytorch.html#introduction-to-neural-networks",
    "href": "intro_pytorch.html#introduction-to-neural-networks",
    "title": "basic-DL",
    "section": "",
    "text": "Neural networks are computational models inspired by the human brain. They consist of layers of interconnected nodes (neurons) where each connection has an associated weight. The basic flow in a neural network involves:\n\nForward Propagation: Input data passes through the network layers to produce an output.\nLoss Calculation: The output is compared with the actual target to compute the loss.\nBackpropagation: The network adjusts the weights to minimize the loss.\n\n\n\n\nA neural network typically has:\n\nInput Layer: Receives the input data.\nHidden Layers: Intermediate layers that process the inputs.\nOutput Layer: Produces the final output.\n\n\n\n\nActivation functions introduce non-linearity into the network, allowing it to learn complex patterns. Common activation functions include:\n\nReLU (Rectified Linear Unit): \\[f(x) = \\max(0, x) \\]\nSigmoid: \\[\\displaystyle f(x) = \\frac{1}{1 + e^{-x}}\\]\nTanh: \\[ f(x) = \\tanh(x) \\]\n\n\n\n\nDuring forward propagation, inputs pass through each layer and activation function to produce the final output. Example of Using the Sigmoid Activation Function: \\[\\displaystyle f(x) = \\frac{1}{1 + e^{-(w*x+b)}}\\]\nwhere: \\(w\\) is the weight and \\(b\\) is the bias.\n\n\n\nThe loss function measures the difference between the predicted output and the actual target. Common loss functions include:\n\nMean Squared Error (MSE) for regression tasks.\nCross-Entropy Loss for classification tasks.\n\n\n\n\nBackpropagation updates the weights using the gradients of the loss function with respect to the weights."
  },
  {
    "objectID": "intro_pytorch.html#loss-function-and-optimization",
    "href": "intro_pytorch.html#loss-function-and-optimization",
    "title": "basic-DL",
    "section": "Loss Function and Optimization",
    "text": "Loss Function and Optimization\nIn machine learning, the loss function (or cost function) measures how well the model’s predictions match the actual target values. The goal of training a neural network is to minimize this loss function.\n\nLocal Minimum vs. Global Minimum\n\nGlobal Minimum: This is the lowest possible point of the loss function. If we reach this point, the model parameters are optimal, and the model performs best on the training data.\nLocal Minimum: These are points where the loss function has a lower value than in its immediate vicinity, but not the lowest possible value. There can be multiple local minima.\n\n\n\nChallenges with Local Minima\n\nTrapped in Local Minima: During training, the optimization algorithm might get trapped in a local minimum, preventing the model from reaching the global minimum.\nPoor Generalization: Models trained in local minima might not generalize well to unseen data.\n\n\n\nWhy Use the Adam Optimizer?\n\nThe Adam (Adaptive Moment Estimation) optimizer is an advanced optimization algorithm that combines the benefits of two other extensions of stochastic gradient descent: Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). Here’s why Adam is beneficial:\n\nAdaptive Learning Rates: Adam adjusts the learning rate for each parameter dynamically based on estimates of lower-order moments. This allows for efficient and effective training.\nMomentum: Adam incorporates the concept of momentum, which helps to smooth out the update process by considering the past gradients, thus potentially avoiding local minima.\nBias Correction: Adam includes bias-correction mechanisms to ensure that the moment estimates are unbiased, particularly during the initial steps of training.\nComputational Efficiency: Adam is computationally efficient and has low memory requirements, making it suitable for large-scale data and models.\nRobustness: Adam works well with noisy data and sparse gradients, which are common in practical deep learning applications.\n\n\n\nGeneral Structure of Constructing and Training a Deep Learning Model\nHere are the general steps to build and train a deep learning model:\n\nDefine the Model Architecture\n\nSpecify the type of model (e.g., feedforward neural network, convolutional neural network).\nDefine the number of layers and the type of each layer (e.g., fully connected, convolutional, dropout).\nChoose activation functions for each layer (e.g., ReLU, sigmoid).\n\nPrepare the Data\n\nCollect and preprocess the data (e.g., normalization, resizing, augmentation).\nSplit the data into training, validation, and test sets.\n\nDefine the Loss Function and Optimizer\n\nChoose a loss function appropriate for the task (e.g., cross-entropy for classification, mean squared error for regression).\nSelect an optimization algorithm (e.g., SGD, Adam).\n\nImplement the Training Loop\n\nInitialize model parameters.\nFor a specified number of epochs:\n\nForward Pass: Pass the input data through the model to get the output.\nCompute Loss: Calculate the loss using the model’s output and the actual target values.\nBackward Pass: Compute gradients by backpropagation.\nUpdate Parameters: Update the model’s parameters using the optimizer.\nOptionally, compute and log metrics on the validation set to monitor training progress.\n\n\nEvaluate the Model\n\nAfter training, evaluate the model on the test set to assess its performance.\n\nFine-tune and Deploy\n\nFine-tune the model by adjusting hyperparameters, adding regularization, or using more advanced architectures.\nOnce satisfied with the performance, deploy the model for inference on new data."
  },
  {
    "objectID": "DL-detail.html#loss-function-and-optimization",
    "href": "DL-detail.html#loss-function-and-optimization",
    "title": "1. Initialize the weights and biases",
    "section": "Loss Function and Optimization",
    "text": "Loss Function and Optimization\nIn machine learning, the loss function (or cost function) measures how well the model’s predictions match the actual target values. The goal of training a neural network is to minimize this loss function.\n\nLocal Minimum vs. Global Minimum\n\nGlobal Minimum: This is the lowest possible point of the loss function. If we reach this point, the model parameters are optimal, and the model performs best on the training data.\nLocal Minimum: These are points where the loss function has a lower value than in its immediate vicinity, but not the lowest possible value. There can be multiple local minima.\n\n\n\nChallenges with Local Minima\n\nTrapped in Local Minima: During training, the optimization algorithm might get trapped in a local minimum, preventing the model from reaching the global minimum.\nPoor Generalization: Models trained in local minima might not generalize well to unseen data.\n\n\n\nWhy Use the Adam Optimizer?\n\nThe Adam (Adaptive Moment Estimation) optimizer is an advanced optimization algorithm that combines the benefits of two other extensions of stochastic gradient descent: Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). Here’s why Adam is beneficial:\n\nAdaptive Learning Rates: Adam adjusts the learning rate for each parameter dynamically based on estimates of lower-order moments. This allows for efficient and effective training.\nMomentum: Adam incorporates the concept of momentum, which helps to smooth out the update process by considering the past gradients, thus potentially avoiding local minima.\nBias Correction: Adam includes bias-correction mechanisms to ensure that the moment estimates are unbiased, particularly during the initial steps of training.\nComputational Efficiency: Adam is computationally efficient and has low memory requirements, making it suitable for large-scale data and models.\nRobustness: Adam works well with noisy data and sparse gradients, which are common in practical deep learning applications.\n\n\n\nGeneral Structure of Constructing and Training a Deep Learning Model\n\nDefine the Model Architecture\n\nSpecify the type of model (e.g., feedforward neural network, convolutional neural network).\nDefine the number of layers and the type of each layer (e.g., fully connected, convolutional, dropout).\nChoose activation functions for each layer (e.g., ReLU, sigmoid).\n\nPrepare the Data\n\nCollect and preprocess the data (e.g., normalization, resizing, augmentation).\nSplit the data into training, validation, and test sets.\n\nDefine the Loss Function and Optimizer\n\nChoose a loss function appropriate for the task (e.g., cross-entropy for classification, mean squared error for regression).\nSelect an optimization algorithm (e.g., SGD, Adam).\n\nImplement the Training Loop\n\nInitialize model parameters.\nFor a specified number of epochs:\n\nForward Pass: Pass the input data through the model to get the output.\nCompute Loss: Calculate the loss using the model’s output and the actual target values.\nBackward Pass: Compute gradients by backpropagation.\nUpdate Parameters: Update the model’s parameters using the optimizer.\nOptionally, compute and log metrics on the validation set to monitor training progress.\n\n\nEvaluate the Model\n\nAfter training, evaluate the model on the test set to assess its performance.\n\nFine-tune and Deploy\n\nFine-tune the model by adjusting hyperparameters, adding regularization, or using more advanced architectures.\nOnce satisfied with the performance, deploy the model for inference on new data."
  },
  {
    "objectID": "DL-detail.html#local-minimum-vs.-global-minimum",
    "href": "DL-detail.html#local-minimum-vs.-global-minimum",
    "title": "1. Initialize the weights and biases",
    "section": "Local Minimum vs. Global Minimum",
    "text": "Local Minimum vs. Global Minimum\n\nGlobal Minimum: This is the lowest possible point of the loss function. If we reach this point, the model parameters are optimal, and the model performs best on the training data.\nLocal Minimum: These are points where the loss function has a lower value than in its immediate vicinity, but not the lowest possible value. There can be multiple local minima."
  },
  {
    "objectID": "DL-detail.html#challenges-with-local-minima",
    "href": "DL-detail.html#challenges-with-local-minima",
    "title": "1. Initialize the weights and biases",
    "section": "Challenges with Local Minima",
    "text": "Challenges with Local Minima\n\nTrapped in Local Minima: During training, the optimization algorithm might get trapped in a local minimum, preventing the model from reaching the global minimum.\nPoor Generalization: Models trained in local minima might not generalize well to unseen data."
  },
  {
    "objectID": "DL-detail.html#why-use-the-adam-optimizer",
    "href": "DL-detail.html#why-use-the-adam-optimizer",
    "title": "1. Initialize the weights and biases",
    "section": "Why Use the Adam Optimizer?",
    "text": "Why Use the Adam Optimizer?\n\nThe Adam (Adaptive Moment Estimation) optimizer is an advanced optimization algorithm that combines the benefits of two other extensions of stochastic gradient descent: Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). Here’s why Adam is beneficial:\n\nAdaptive Learning Rates: Adam adjusts the learning rate for each parameter dynamically based on estimates of lower-order moments. This allows for efficient and effective training.\nMomentum: Adam incorporates the concept of momentum, which helps to smooth out the update process by considering the past gradients, thus potentially avoiding local minima.\nBias Correction: Adam includes bias-correction mechanisms to ensure that the moment estimates are unbiased, particularly during the initial steps of training.\nComputational Efficiency: Adam is computationally efficient and has low memory requirements, making it suitable for large-scale data and models.\nRobustness: Adam works well with noisy data and sparse gradients, which are common in practical deep learning applications."
  },
  {
    "objectID": "DL-detail.html#common-loss-functions",
    "href": "DL-detail.html#common-loss-functions",
    "title": "1. Initialize the weights and biases",
    "section": "Common Loss Functions",
    "text": "Common Loss Functions\n\nMean Squared Error Loss (MSELoss):\n\nUsed for regression tasks.\nMeasures the average squared difference between the predicted and actual values.\n\nCross-Entropy Loss (CrossEntropyLoss):\n\nUsed for classification tasks.\nCombines LogSoftmax and NLLLoss in one single class.\n\nBinary Cross-Entropy Loss (BCELoss):\n\nUsed for binary classification tasks.\nMeasures the binary cross-entropy between the target and the output.\n\nNegative Log-Likelihood Loss (NLLLoss):\n\nUsed for classification tasks.\nOften used with LogSoftmax.\n\nHinge Embedding Loss (HingeEmbeddingLoss):\n\nUsed for binary classification tasks where the output is either 1 or -1."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Basis Deep Learning Workshop",
    "section": "",
    "text": "Our deep learning workshop includes core concepts with hands-on PyTorch experience. You’ll dive into Machine Learning (ML) and Deep Learning (DL) basics, focusing on Artificial Neural Networks (ANN). Explore astrophysics applications and participate in interactive Q&A sessions. Learn about forward and backward passes, layers, activation functions, and loss calculation. Engage in practical exercises, such as fitting a sine function in Google Colab, and discover Tensor Playground to visualize tensor operations."
  },
  {
    "objectID": "index.html#timetable",
    "href": "index.html#timetable",
    "title": "Basis Deep Learning Workshop",
    "section": "",
    "text": "Time\nActivity\n\n\n\n\n12:30 - 13:00\nCoffee\n\n\n13:00 - 13:05\nIntroduction to the Workshop: Welcome\n\n\n13:05 - 13:50\nIntroduction to ML and DL (with ANN)\n\n\n\nML and DL applied in Astrophysics\n\n\n\nQ&A\n\n\n13:50 - 14:00\nCoffee break\n\n\n14:00 - 14:30\nIntro: Layers, Activations, Backpropagation, Loss\n\n\n14:30 - 15:20\nWhat is the neural network and how it learns\n\n\n\nDeep Learning with Pytorch and Tensor Playground\n\n\n15:20 - 15:25\nSummary: General structure of constructing a model using DL\n\n\n15:25 - 15:40\nQ&A\n\n\n15:40 - 15:55\nPlaying with Tensor Playground\n\n\n15:55 - 16:00\nQ&A\n\n\n16:00 - 16:15\nCoffee break\n\n\n16:15 - 17:30\nHands-on session"
  },
  {
    "objectID": "DL-intro.html#applications-of-deep-learning-in-astrophysics",
    "href": "DL-intro.html#applications-of-deep-learning-in-astrophysics",
    "title": "basic-DL",
    "section": "Applications of Deep Learning in Astrophysics",
    "text": "Applications of Deep Learning in Astrophysics\n\n1. Observation of Interstellar Medium (ISM) and Molecular Clouds\n\nData Reduction and Noise Filtering: ML algorithms enhance observational data quality by reducing noise and improving the signal-to-noise ratio in data from radio, infrared, and optical telescopes.\n\nReference: Samuel, H. S., Etim, E. E., Shinggu, J. P., & Bako, B. (2023). Machine learning of Rotational spectra analysis in interstellar medium. Communication in Physical Sciences, 10(1), 172-203.\n\nFeature Extraction: DL models, such as convolutional neural networks (CNNs), identify and classify structures within the ISM and molecular clouds, such as filaments and cores.\n\nReference: Samuel, H. S., Etim, E. E., Shinggu, J. P., & Bako, B. (2023). Machine learning of Rotational spectra analysis in interstellar medium. Communication in Physical Sciences, 10(1), 172-203.\n\nAnomaly Detection: ML techniques detect unusual features or events within the ISM, potentially indicating new astrophysical phenomena.\n\nReference: Samuel, H. S., Etim, E. E., Shinggu, J. P., & Bako, B. (2023). Machine learning of Rotational spectra analysis in interstellar medium. Communication in Physical Sciences, 10(1), 172-203.\n\n\n\n\n2. High-Mass Star Formation\n\nData Analysis: ML models analyze large datasets from observatories to identify regions of high-mass star formation by detecting specific spectral lines and emission features.\n\nReference: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. Monthly Notices of the Royal Astronomical Society, 493(4), 4808-4815.\n\nPattern Recognition: DL algorithms identify patterns and correlations in the spatial and temporal distribution of high-mass star-forming regions, helping to understand the underlying physical processes.\n\nReference: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. Monthly Notices of the Royal Astronomical Society, 493(4), 4808-4815.\n\nPredictive Modeling: ML models predict the future evolution of star-forming regions based on current observations.\n\nReference: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. Monthly Notices of the Royal Astronomical Society, 493(4), 4808-4815.\n\n\n\n\n3. Simulation of High-Mass Star Formation\n\nAccelerated Simulations: ML techniques, such as generative adversarial networks (GANs), enhance the efficiency of high-mass star formation simulations by generating high-fidelity results from lower-resolution inputs.\n\nReference: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. Astronomy & Astrophysics, 621, A51.\n\nParameter Space Exploration: ML models explore the vast parameter space of star formation simulations to identify optimal conditions and initial configurations for high-mass star formation.\n\nReference: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. Astronomy & Astrophysics, 621, A51.\n\n\n\n\n4. Astrochemistry\n\nChemical Abundance Mapping: ML models analyze spectral data to map the distribution of various molecules within molecular clouds and other astrophysical environments.\n\nReference: Xue, B.-X., Barbatti, M., & Dral, P. (2020). Machine learning for absorption cross sections. Journal of Physical Chemistry A, 124(35), 7199-7210.\n\nReaction Network Analysis: DL helps in understanding complex chemical networks by identifying key reactions and pathways in the formation and destruction of molecules in space.\n\nReference: Villadsen, T., Ligterink, N. F. W., & Andersen, M. (2022). Predicting binding energies of astrochemically relevant molecules via machine learning. Astronomy & Astrophysics, 666, A45.\n\nAnomaly Detection: ML algorithms identify unusual chemical compositions or rare molecules, providing insights into unique astrochemical processes.\n\nReference: Lee, K. L. K., Patterson, J., Burkhardt, A. M., Vankayalapati, V., & McCarthy, M. C. (2021). Machine learning of interstellar chemical inventories. Astrophysical Journal Letters, 917(1), L6.\n\n\n\n\n5. Stellar Feedback\n\nSimulation and Modeling: DL accelerates simulations of stellar feedback processes, such as supernova explosions and stellar winds, by learning from high-resolution simulations and predicting outcomes for new scenarios.\n\nReference: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. Monthly Notices of the Royal Astronomical Society, 493(4), 4808-4815.\n\nImpact Analysis: ML techniques assess the impact of stellar feedback on surrounding ISM and star formation by analyzing observational data and simulation outputs.\n\nReference: Surana, S., Wadadekar, Y., Bait, O., & Bhosale, H. (2020). Predicting star formation properties of galaxies using deep learning. Monthly Notices of the Royal Astronomical Society, 493(4), 4808-4815.\n\n\n\n\n6. Galaxy Formation\n\nMorphological Classification: CNNs classify galaxies based on their shapes and structures from large-scale survey images, aiding in the study of galaxy formation and evolution.\n\nReference: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. Astronomy & Astrophysics, 621, A51.\n\nMerger Detection: ML algorithms detect galaxy mergers and interactions in observational data and simulations, which are crucial for understanding galaxy formation processes.\n\nReference: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. Astronomy & Astrophysics, 621, A51.\n\nPredictive Modeling: DL models predict the future evolution of galaxies based on current observations and simulations, helping to test theories of galaxy formation.\n\nReference: Bonjean, V., Aghanim, N., Salomé, P., Beelen, A., Douspis, M., & Soubrié, E. (2019). Star formation rates and stellar masses from machine learning. Astronomy & Astrophysics, 621, A51."
  }
]